{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "try : \n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "except : \n",
    "    pass\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "        self.device = device\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "import torch\n",
    "\n",
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    state= np.log(state+1e-9)\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).to(device))\n",
    "        return torch.argmax(Q, dim=1).cpu()\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "class dqn_agent:\n",
    "    def __init__(self, config, model,pi =None):\n",
    "        self.pi = pi\n",
    "        device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.memory = ReplayBuffer(buffer_size,device)\n",
    "        self.epsilon_max = config['epsilon_max'] if 'epsilon_max' in config.keys() else 1.\n",
    "        self.epsilon_min = config['epsilon_min'] if 'epsilon_min' in config.keys() else 0.01\n",
    "        self.epsilon_stop = config['epsilon_decay_period'] if 'epsilon_decay_period' in config.keys() else 1000\n",
    "        self.epsilon_delay = config['epsilon_delay_decay'] if 'epsilon_delay_decay' in config.keys() else 20\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.model = model \n",
    "        self.target_model = deepcopy(self.model).to(device)\n",
    "        self.criterion = config['criterion'] if 'criterion' in config.keys() else torch.nn.MSELoss()\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = config['optimizer'] if 'optimizer' in config.keys() else torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.nb_gradient_steps = config['gradient_steps'] if 'gradient_steps' in config.keys() else 1\n",
    "        self.update_target_strategy = config['update_target_strategy'] if 'update_target_strategy' in config.keys() else 'replace'\n",
    "        self.update_target_freq = config['update_target_freq'] if 'update_target_freq' in config.keys() else 20\n",
    "        self.update_target_tau = config['update_target_tau'] if 'update_target_tau' in config.keys() else 0.005\n",
    "        self.monitoring_nb_trials = config['monitoring_nb_trials'] if 'monitoring_nb_trials' in config.keys() else 0\n",
    "\n",
    "    def MC_eval(self, env, nb_trials):   # NEW NEW NEW\n",
    "        MC_total_reward = []\n",
    "        MC_discounted_reward = []\n",
    "        for _ in range(nb_trials):\n",
    "            x,_ = env.reset()\n",
    "            done = False\n",
    "            trunc = False\n",
    "            total_reward = 0\n",
    "            discounted_reward = 0\n",
    "            step = 0\n",
    "            while not (done or trunc):\n",
    "                a = greedy_action(self.model, x)\n",
    "                y,r,done,trunc,_ = env.step(a)\n",
    "                x = y\n",
    "                total_reward += r\n",
    "                discounted_reward += self.gamma**step * r\n",
    "                step += 1\n",
    "            MC_total_reward.append(total_reward)\n",
    "            MC_discounted_reward.append(discounted_reward)\n",
    "        return np.mean(MC_discounted_reward), np.mean(MC_total_reward)\n",
    "    \n",
    "    def V_initial_state(self, env, nb_trials):   # NEW NEW NEW\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nb_trials):\n",
    "                val = []\n",
    "                x,_ = env.reset()\n",
    "                val.append(self.model(torch.Tensor(x).unsqueeze(0).to(device)).max().item())\n",
    "        return np.mean(val)\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            X,Y = (X+1e-9).log() , (Y+1e-9).log()\n",
    "            QYmax = self.target_model(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(),0.5)\n",
    "            self.optimizer.step() \n",
    "    \n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        MC_avg_total_reward = []   # NEW NEW NEW\n",
    "        MC_avg_discounted_reward = []   # NEW NEW NEW\n",
    "        V_init_state = []   # NEW NEW NEW\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "            # select epsilon-greedy action\n",
    "            if episode <1 and self.pi is not None:\n",
    "                action = self.pi.get_action(state)\n",
    "            else :\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                    \n",
    "                else:\n",
    "                    action = greedy_action(self.model, state)\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            for u in range(env.observation_space.shape[0]) :\n",
    "                self.memory.append(state[u], action[u], reward[u], next_state[u], done[u])\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_model.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_model.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "                self.target_model.load_state_dict(target_state_dict)\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if any(done) or any(trunc):\n",
    "                episode += 1\n",
    "                # Monitoring\n",
    "                if self.monitoring_nb_trials>0:\n",
    "                    MC_dr, MC_tr = self.MC_eval(env, self.monitoring_nb_trials)    # NEW NEW NEW\n",
    "                    V0 = self.V_initial_state(env, self.monitoring_nb_trials)   # NEW NEW NEW\n",
    "                    MC_avg_total_reward.append(MC_tr)   # NEW NEW NEW\n",
    "                    MC_avg_discounted_reward.append(MC_dr)   # NEW NEW NEW\n",
    "                    V_init_state.append(V0)   # NEW NEW NEW\n",
    "                    episode_return.append(episode_cum_reward)   # NEW NEW NEW\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.memory)), \n",
    "                          \", ep return \", '{:e}'.format(episode_cum_reward), \n",
    "                          \", MC tot \", '{:6.2f}'.format(MC_tr),\n",
    "                          \", MC disc \", '{:6.2f}'.format(MC_dr),\n",
    "                          \", V0 \", '{:6.2f}'.format(V0),\n",
    "                          sep='')\n",
    "                else:\n",
    "                    episode_return.append(episode_cum_reward)\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.memory)), \n",
    "                          \", ep return \", '{:e}'.format(descale(episode_cum_reward.mean())), \n",
    "                          sep='')\n",
    "\n",
    "                \n",
    "                state, _ = env.reset()\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "            if descale(np.mean(episode_cum_reward))>4e10 : \n",
    "                return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, V_init_state\n",
    "        return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, V_init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1, epsilon   0.01, batch size 2000, ep return 3.041952e+10\n",
      "Episode  2, epsilon   0.01, batch size 4000, ep return 3.423050e+10\n",
      "Episode  3, epsilon   0.01, batch size 6000, ep return 3.058209e+10\n",
      "Episode  4, epsilon   0.01, batch size 8000, ep return 2.698586e+10\n",
      "Episode  5, epsilon   0.01, batch size 10000, ep return 2.657617e+10\n",
      "Episode  6, epsilon   0.01, batch size 12000, ep return 2.848544e+10\n",
      "Episode  7, epsilon   0.01, batch size 14000, ep return 2.312518e+10\n",
      "Episode  8, epsilon   0.01, batch size 16000, ep return 2.974235e+10\n",
      "Episode  9, epsilon   0.01, batch size 18000, ep return 2.876981e+10\n",
      "Episode 10, epsilon   0.01, batch size 20000, ep return 2.971053e+10\n",
      "Episode 11, epsilon   0.01, batch size 22000, ep return 2.518730e+10\n",
      "Episode 12, epsilon   0.01, batch size 24000, ep return 2.801637e+10\n",
      "Episode 13, epsilon   0.01, batch size 26000, ep return 3.336907e+10\n",
      "Episode 14, epsilon   0.01, batch size 28000, ep return 3.086545e+10\n",
      "Episode 15, epsilon   0.01, batch size 30000, ep return 2.631098e+10\n",
      "Episode 16, epsilon   0.01, batch size 32000, ep return 3.042181e+10\n",
      "Episode 17, epsilon   0.01, batch size 34000, ep return 2.566530e+10\n",
      "Episode 18, epsilon   0.01, batch size 36000, ep return 3.173163e+10\n",
      "Episode 19, epsilon   0.01, batch size 38000, ep return 2.789224e+10\n",
      "Episode 20, epsilon   0.01, batch size 40000, ep return 2.643546e+10\n",
      "Episode 21, epsilon   0.01, batch size 42000, ep return 2.525152e+10\n",
      "Episode 22, epsilon   0.01, batch size 44000, ep return 2.826913e+10\n",
      "Episode 23, epsilon   0.01, batch size 46000, ep return 2.961655e+10\n",
      "Episode 24, epsilon   0.01, batch size 48000, ep return 2.966689e+10\n",
      "Episode 25, epsilon   0.01, batch size 50000, ep return 2.717292e+10\n",
      "Episode 26, epsilon   0.01, batch size 52000, ep return 2.955848e+10\n",
      "Episode 27, epsilon   0.01, batch size 54000, ep return 2.935518e+10\n",
      "Episode 28, epsilon   0.01, batch size 56000, ep return 2.739758e+10\n",
      "Episode 29, epsilon   0.01, batch size 58000, ep return 2.984510e+10\n",
      "Episode 30, epsilon   0.01, batch size 60000, ep return 3.232940e+10\n",
      "Episode 31, epsilon   0.01, batch size 62000, ep return 2.857114e+10\n",
      "Episode 32, epsilon   0.01, batch size 64000, ep return 2.873783e+10\n",
      "Episode 33, epsilon   0.01, batch size 66000, ep return 2.733248e+10\n",
      "Episode 34, epsilon   0.01, batch size 68000, ep return 2.983673e+10\n",
      "Episode 35, epsilon   0.01, batch size 70000, ep return 3.139450e+10\n",
      "Episode 36, epsilon   0.01, batch size 72000, ep return 3.191104e+10\n",
      "Episode 37, epsilon   0.01, batch size 74000, ep return 2.706820e+10\n",
      "Episode 38, epsilon   0.01, batch size 76000, ep return 3.321114e+10\n",
      "Episode 39, epsilon   0.01, batch size 78000, ep return 3.073754e+10\n",
      "Episode 40, epsilon   0.01, batch size 80000, ep return 2.418280e+10\n",
      "Episode 41, epsilon   0.01, batch size 82000, ep return 2.952733e+10\n",
      "Episode 42, epsilon   0.01, batch size 84000, ep return 3.094928e+10\n",
      "Episode 43, epsilon   0.01, batch size 86000, ep return 2.583612e+10\n",
      "Episode 44, epsilon   0.01, batch size 88000, ep return 2.336322e+10\n",
      "Episode 45, epsilon   0.01, batch size 90000, ep return 3.240902e+10\n",
      "Episode 46, epsilon   0.01, batch size 92000, ep return 3.147267e+10\n",
      "Episode 47, epsilon   0.01, batch size 94000, ep return 3.150550e+10\n",
      "Episode 48, epsilon   0.01, batch size 96000, ep return 2.912671e+10\n",
      "Episode 49, epsilon   0.01, batch size 98000, ep return 2.575788e+10\n",
      "Episode 50, epsilon   0.01, batch size 100000, ep return 3.323070e+10\n",
      "Episode 51, epsilon   0.01, batch size 102000, ep return 2.833606e+10\n",
      "Episode 52, epsilon   0.01, batch size 104000, ep return 2.744757e+10\n",
      "Episode 53, epsilon   0.01, batch size 106000, ep return 2.934906e+10\n",
      "Episode 54, epsilon   0.01, batch size 108000, ep return 2.998795e+10\n",
      "Episode 55, epsilon   0.01, batch size 110000, ep return 3.277700e+10\n",
      "Episode 56, epsilon   0.01, batch size 112000, ep return 2.836907e+10\n",
      "Episode 57, epsilon   0.01, batch size 114000, ep return 3.036372e+10\n",
      "Episode 58, epsilon   0.01, batch size 116000, ep return 3.124732e+10\n",
      "Episode 59, epsilon   0.01, batch size 118000, ep return 3.289491e+10\n",
      "Episode 60, epsilon   0.01, batch size 120000, ep return 2.927733e+10\n",
      "Episode 61, epsilon   0.01, batch size 122000, ep return 2.917912e+10\n",
      "Episode 62, epsilon   0.01, batch size 124000, ep return 2.879195e+10\n",
      "Episode 63, epsilon   0.01, batch size 126000, ep return 2.484756e+10\n",
      "Episode 64, epsilon   0.01, batch size 128000, ep return 2.399355e+10\n",
      "Episode 65, epsilon   0.01, batch size 130000, ep return 3.133566e+10\n",
      "Episode 66, epsilon   0.01, batch size 132000, ep return 2.952526e+10\n",
      "Episode 67, epsilon   0.01, batch size 134000, ep return 2.920532e+10\n",
      "Episode 68, epsilon   0.01, batch size 136000, ep return 3.033897e+10\n",
      "Episode 69, epsilon   0.01, batch size 138000, ep return 2.851775e+10\n",
      "Episode 70, epsilon   0.01, batch size 140000, ep return 2.570110e+10\n",
      "Episode 71, epsilon   0.01, batch size 142000, ep return 3.368882e+10\n",
      "Episode 72, epsilon   0.01, batch size 144000, ep return 2.842980e+10\n",
      "Episode 73, epsilon   0.01, batch size 146000, ep return 2.937165e+10\n",
      "Episode 74, epsilon   0.01, batch size 148000, ep return 2.862408e+10\n",
      "Episode 75, epsilon   0.01, batch size 150000, ep return 3.187098e+10\n",
      "Episode 76, epsilon   0.01, batch size 152000, ep return 3.030966e+10\n",
      "Episode 77, epsilon   0.01, batch size 154000, ep return 3.349834e+10\n",
      "Episode 78, epsilon   0.01, batch size 156000, ep return 3.176262e+10\n",
      "Episode 79, epsilon   0.01, batch size 158000, ep return 2.435157e+10\n",
      "Episode 80, epsilon   0.01, batch size 160000, ep return 2.853217e+10\n",
      "Episode 81, epsilon   0.01, batch size 162000, ep return 2.703830e+10\n",
      "Episode 82, epsilon   0.01, batch size 164000, ep return 2.808859e+10\n",
      "Episode 83, epsilon   0.01, batch size 166000, ep return 2.902247e+10\n",
      "Episode 84, epsilon   0.01, batch size 168000, ep return 2.515371e+10\n",
      "Episode 85, epsilon   0.01, batch size 170000, ep return 3.135992e+10\n",
      "Episode 86, epsilon   0.01, batch size 172000, ep return 2.637511e+10\n",
      "Episode 87, epsilon   0.01, batch size 174000, ep return 2.988511e+10\n",
      "Episode 88, epsilon   0.01, batch size 176000, ep return 2.996806e+10\n",
      "Episode 89, epsilon   0.01, batch size 178000, ep return 3.006941e+10\n",
      "Episode 90, epsilon   0.01, batch size 180000, ep return 3.279503e+10\n",
      "Episode 91, epsilon   0.01, batch size 182000, ep return 3.103117e+10\n",
      "Episode 92, epsilon   0.01, batch size 184000, ep return 3.058852e+10\n",
      "Episode 93, epsilon   0.01, batch size 186000, ep return 2.461434e+10\n",
      "Episode 94, epsilon   0.01, batch size 188000, ep return 2.989139e+10\n",
      "Episode 95, epsilon   0.01, batch size 190000, ep return 2.517252e+10\n",
      "Episode 96, epsilon   0.01, batch size 192000, ep return 3.024286e+10\n",
      "Episode 97, epsilon   0.01, batch size 194000, ep return 2.966191e+10\n",
      "Episode 98, epsilon   0.01, batch size 196000, ep return 2.779003e+10\n",
      "Episode 99, epsilon   0.01, batch size 198000, ep return 3.352428e+10\n",
      "Episode 100, epsilon   0.01, batch size 200000, ep return 2.706323e+10\n",
      "Episode 101, epsilon   0.01, batch size 202000, ep return 2.947244e+10\n",
      "Episode 102, epsilon   0.01, batch size 204000, ep return 3.193471e+10\n",
      "Episode 103, epsilon   0.01, batch size 206000, ep return 2.693380e+10\n",
      "Episode 104, epsilon   0.01, batch size 208000, ep return 2.642910e+10\n",
      "Episode 105, epsilon   0.01, batch size 210000, ep return 3.023257e+10\n",
      "Episode 106, epsilon   0.01, batch size 212000, ep return 2.831928e+10\n",
      "Episode 107, epsilon   0.01, batch size 214000, ep return 3.263566e+10\n",
      "Episode 108, epsilon   0.01, batch size 216000, ep return 3.308353e+10\n",
      "Episode 109, epsilon   0.01, batch size 218000, ep return 2.803917e+10\n",
      "Episode 110, epsilon   0.01, batch size 220000, ep return 3.254363e+10\n",
      "Episode 111, epsilon   0.01, batch size 222000, ep return 3.036039e+10\n",
      "Episode 112, epsilon   0.01, batch size 224000, ep return 2.840352e+10\n",
      "Episode 113, epsilon   0.01, batch size 226000, ep return 2.822875e+10\n",
      "Episode 114, epsilon   0.01, batch size 228000, ep return 2.839198e+10\n",
      "Episode 115, epsilon   0.01, batch size 230000, ep return 2.363355e+10\n",
      "Episode 116, epsilon   0.01, batch size 232000, ep return 2.913629e+10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m agent\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m deepcopy(dqn\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     67\u001b[0m agent\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m deepcopy(dqn\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 68\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m, in \u001b[0;36mdqn_agent.train\u001b[0;34m(self, env, max_episode)\u001b[0m\n\u001b[1;32m    128\u001b[0m         action \u001b[38;5;241m=\u001b[39m greedy_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, state)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# step\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m next_state, reward, done, trunc, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) :\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mappend(state[u], action[u], reward[u], next_state[u], done[u])\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/site-packages/gymnasium/vector/vector_env.py:204\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    {}\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/site-packages/gymnasium/vector/async_vector_env.py:322\u001b[0m, in \u001b[0;36mAsyncVectorEnv.step_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    320\u001b[0m successes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent_pipes):\n\u001b[0;32m--> 322\u001b[0m     result, success \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     successes\u001b[38;5;241m.\u001b[39mappend(success)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import env_hiv\n",
    "from typing import Protocol\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Declare network\n",
    "from env_hiv import HIVPatient\n",
    "env = HIVPatient(domain_randomization=True)\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from gymnasium.wrappers import TransformReward\n",
    "def rew(state,action, env) : \n",
    "    return-(\n",
    "                env.Q * state[4]\n",
    "                + env.R1 * action[0] ** 2\n",
    "                + env.R2 * action[1] ** 2\n",
    "                - env.S * state[5]\n",
    "            )\n",
    "low_r, high_r = rew(env.lower, [1,1],env),rew(env.upper, [1,1],env)\n",
    "env = TransformReward(env, lambda r: (r-low_r)/(high_r-low_r))\n",
    "env = TimeLimit(env,200)\n",
    "env =  gym.vector.AsyncVectorEnv([lambda : env for i in range(10)])\n",
    "def descale(r) :\n",
    "    return r * (high_r - low_r) +low_r\n",
    "\n",
    "state_dim = env.observation_space.shape[1]\n",
    "n_action = env.action_space.nvec[0]\n",
    "nb_neurons=256\n",
    "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "\n",
    "config = {'nb_actions': n_action,\n",
    "          'learning_rate': 1e-2,\n",
    "          'gamma': 0.9,\n",
    "          'buffer_size': 1000000,\n",
    "          'epsilon_min': 0.01,\n",
    "          'epsilon_max': 0.01,\n",
    "          'epsilon_decay_period': 1000,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'batch_size': 1024,\n",
    "          'gradient_steps':3,\n",
    "          'update_target_strategy': 'replace', # or 'ema'\n",
    "          'update_target_freq': 50,\n",
    "          'update_target_tau': 0.005,\n",
    "          'criterion': torch.nn.SmoothL1Loss(),\n",
    "          'monitoring_nb_trials': 0}\n",
    "\n",
    "from train_cma import ProjectAgent\n",
    "with open('DQNAGENTS/saved3.pkl', 'rb') as f:  # open a text file\n",
    "    dqn = pickle.load( f)['dqn'] # serialize the list\n",
    "agent = dqn_agent(config, DQN)\n",
    "agent.model = deepcopy(dqn.to(device))\n",
    "agent.target_model = deepcopy(dqn.to(device))\n",
    "agent.train(env, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    state= np.log(state+1e-9)\n",
    "    if len(torch.Tensor(state).shape)==1 :\n",
    "        state = torch.Tensor(state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).to(device))\n",
    "        return torch.argmax(Q, dim=1).cpu()\n",
    "class ProjectAgent:\n",
    "\n",
    "    def __init__(self) :\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env =env_hiv.HIVPatient(domain_randomization=False)\n",
    "        self.config = config\n",
    "        self.dqn = None\n",
    "    def act(self, observation: np.ndarray, use_random: bool = False) -> int:\n",
    "        return greedy_action(self.dqn, observation)\n",
    "\n",
    "    def save(self, path=\"\"):\n",
    "        serialized= {\"dqn\":self.dqn.cpu(), \"config\":self.config}\n",
    "        with open('DQNAGENTS/saved3.pkl', 'wb') as f:  # open a text file\n",
    "            pickle.dump(serialized, f) # serialize the list\n",
    "    def load(self):\n",
    "        with open('DQNAGENTS/saved3.pkl', 'rb') as f:  # open a text file\n",
    "            saved = pickle.load( f) # serialize the list\n",
    "        self.dqn= saved[\"dqn\"].to(self.device)\n",
    "        try : \n",
    "            x,_ = self.env.reset()\n",
    "            self.act(x)\n",
    "        except : \n",
    "            raise Exception(\"Actor incompatible with environnement\")\n",
    "    \n",
    "    def train(self):\n",
    "        return self.agent.train(self.env,self.config['epochs'])\n",
    "Pagent = ProjectAgent()\n",
    "Pagent.dqn = agent.model\n",
    "\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.516853e+10 3.248472e+10\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=Pagent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=Pagent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")\n",
    "print(\"{:e}\".format(score_agent),\"{:e}\".format(score_agent_dr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "names = os.listdir(\"DQNAGENTS/\")\n",
    "models= []\n",
    "for name in names :\n",
    "    with open(\"DQNAGENTS/\"+name, 'rb') as f:  # open a text file\n",
    "        models.append(pickle.load( f)['dqn']) # serialize the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProjectAgent:\n",
    "    def __init__(self) :\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env =env_hiv.HIVPatient(domain_randomization=False)\n",
    "        self.config = config\n",
    "        self.models = []\n",
    "    def act(self, observation: np.ndarray, use_random: bool = False) -> int:\n",
    "        observation = (torch.Tensor(observation).to(torch.float32)+1e-9).log()\n",
    "        logits = [m(observation) for m in self.models[:3]]\n",
    "        #logits =[models[0](observation)]+[models[2](observation)]\n",
    "        logits =torch.softmax(torch.stack(logits,0),1).mean(0)\n",
    "        return torch.argmax(logits).item()\n",
    "\n",
    "    def save(self, path=\"\"):\n",
    "        serialized= {\"models\" :self.models}\n",
    "        with open(path+'models.pkl', 'wb') as f:  # open a text file\n",
    "            pickle.dump(serialized, f) # serialize the list\n",
    "    def load(self):\n",
    "        with open('models.pkl', 'rb') as f:  # open a text file\n",
    "            saved = pickle.load( f) # serialize the list\n",
    "        self.models= saved[\"models\"]\n",
    "        try : \n",
    "            x,_ = self.env.reset()\n",
    "            self.act(x)\n",
    "        except : \n",
    "            raise Exception(\"Actor incompatible with environnement\")\n",
    "agent = ProjectAgent()\n",
    "agent.models= models\n",
    "agent.save()\n",
    "agent.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.877342e+10 2.528770e+10\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")\n",
    "print(\"{:e}\".format(score_agent),\"{:e}\".format(score_agent_dr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved2.pkl', 'saved0.pkl', 'saved3.pkl', 'saved1.pkl']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
