{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "try : \n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "except : \n",
    "    pass\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "        self.device = device\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "import torch\n",
    "\n",
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    state= np.log(state+1e-9)\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).to(device))\n",
    "        return torch.argmax(Q, dim=1).cpu()\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "class dqn_agent:\n",
    "    def __init__(self, config, model,pi =None):\n",
    "        self.pi = pi\n",
    "        device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.memory = ReplayBuffer(buffer_size,device)\n",
    "        self.epsilon_max = config['epsilon_max'] if 'epsilon_max' in config.keys() else 1.\n",
    "        self.epsilon_min = config['epsilon_min'] if 'epsilon_min' in config.keys() else 0.01\n",
    "        self.epsilon_stop = config['epsilon_decay_period'] if 'epsilon_decay_period' in config.keys() else 1000\n",
    "        self.epsilon_delay = config['epsilon_delay_decay'] if 'epsilon_delay_decay' in config.keys() else 20\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.model = model \n",
    "        self.target_model = deepcopy(self.model).to(device)\n",
    "        self.criterion = config['criterion'] if 'criterion' in config.keys() else torch.nn.MSELoss()\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = config['optimizer'] if 'optimizer' in config.keys() else torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.nb_gradient_steps = config['gradient_steps'] if 'gradient_steps' in config.keys() else 1\n",
    "        self.update_target_strategy = config['update_target_strategy'] if 'update_target_strategy' in config.keys() else 'replace'\n",
    "        self.update_target_freq = config['update_target_freq'] if 'update_target_freq' in config.keys() else 20\n",
    "        self.update_target_tau = config['update_target_tau'] if 'update_target_tau' in config.keys() else 0.005\n",
    "        self.monitoring_nb_trials = config['monitoring_nb_trials'] if 'monitoring_nb_trials' in config.keys() else 0\n",
    "\n",
    "    def MC_eval(self, env, nb_trials):   # NEW NEW NEW\n",
    "        MC_total_reward = []\n",
    "        MC_discounted_reward = []\n",
    "        for _ in range(nb_trials):\n",
    "            x,_ = env.reset()\n",
    "            done = False\n",
    "            trunc = False\n",
    "            total_reward = 0\n",
    "            discounted_reward = 0\n",
    "            step = 0\n",
    "            while not (done or trunc):\n",
    "                a = greedy_action(self.model, x)\n",
    "                y,r,done,trunc,_ = env.step(a)\n",
    "                x = y\n",
    "                total_reward += r\n",
    "                discounted_reward += self.gamma**step * r\n",
    "                step += 1\n",
    "            MC_total_reward.append(total_reward)\n",
    "            MC_discounted_reward.append(discounted_reward)\n",
    "        return np.mean(MC_discounted_reward), np.mean(MC_total_reward)\n",
    "    \n",
    "    def V_initial_state(self, env, nb_trials):   # NEW NEW NEW\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nb_trials):\n",
    "                val = []\n",
    "                x,_ = env.reset()\n",
    "                val.append(self.model(torch.Tensor(x).unsqueeze(0).to(device)).max().item())\n",
    "        return np.mean(val)\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            X,Y = (X+1e-9).log() , (Y+1e-9).log()\n",
    "            QYmax = self.target_model(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(),0.5)\n",
    "            self.optimizer.step() \n",
    "    \n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        MC_avg_total_reward = []   # NEW NEW NEW\n",
    "        MC_avg_discounted_reward = []   # NEW NEW NEW\n",
    "        V_init_state = []   # NEW NEW NEW\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "            # select epsilon-greedy action\n",
    "            if episode <1 and self.pi is not None:\n",
    "                action = self.pi.get_action(state)\n",
    "            else :\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                    \n",
    "                else:\n",
    "                    action = greedy_action(self.model, state)\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            for u in range(env.observation_space.shape[0]) :\n",
    "                self.memory.append(state[u], action[u], reward[u], next_state[u], done[u])\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_model.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_model.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "                self.target_model.load_state_dict(target_state_dict)\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if any(done) or any(trunc):\n",
    "                episode += 1\n",
    "                if episode ==1  and self.pi is not None:\n",
    "                    for i in range(50000):\n",
    "                        self.gradient_step()\n",
    "                # Monitoring\n",
    "                if self.monitoring_nb_trials>0:\n",
    "                    MC_dr, MC_tr = self.MC_eval(env, self.monitoring_nb_trials)    # NEW NEW NEW\n",
    "                    V0 = self.V_initial_state(env, self.monitoring_nb_trials)   # NEW NEW NEW\n",
    "                    MC_avg_total_reward.append(MC_tr)   # NEW NEW NEW\n",
    "                    MC_avg_discounted_reward.append(MC_dr)   # NEW NEW NEW\n",
    "                    V_init_state.append(V0)   # NEW NEW NEW\n",
    "                    episode_return.append(episode_cum_reward)   # NEW NEW NEW\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.memory)), \n",
    "                          \", ep return \", '{:e}'.format(episode_cum_reward), \n",
    "                          \", MC tot \", '{:6.2f}'.format(MC_tr),\n",
    "                          \", MC disc \", '{:6.2f}'.format(MC_dr),\n",
    "                          \", V0 \", '{:6.2f}'.format(V0),\n",
    "                          sep='')\n",
    "                else:\n",
    "                    episode_return.append(episode_cum_reward)\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.memory)), \n",
    "                          \", ep return \", '{:e}'.format(descale(episode_cum_reward.mean())), \n",
    "                          sep='')\n",
    "\n",
    "                \n",
    "                state, _ = env.reset()\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "            if descale(np.mean(episode_cum_reward))>4e10 : \n",
    "                return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, V_init_state\n",
    "        return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, V_init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1, epsilon   0.82, batch size 2000, ep return 4.289876e+09\n",
      "Episode  2, epsilon   0.62, batch size 4000, ep return 1.430652e+07\n",
      "Episode  3, epsilon   0.43, batch size 6000, ep return 1.475125e+07\n",
      "Episode  4, epsilon   0.23, batch size 8000, ep return 1.643149e+07\n",
      "Episode  5, epsilon   0.03, batch size 10000, ep return 5.457618e+07\n",
      "Episode  6, epsilon   0.01, batch size 12000, ep return 9.689965e+08\n",
      "Episode  7, epsilon   0.01, batch size 14000, ep return 4.203882e+09\n",
      "Episode  8, epsilon   0.01, batch size 16000, ep return 1.142272e+10\n",
      "Episode  9, epsilon   0.01, batch size 18000, ep return 2.014714e+10\n",
      "Episode 10, epsilon   0.01, batch size 20000, ep return 5.231815e+09\n",
      "Episode 11, epsilon   0.01, batch size 22000, ep return 7.775940e+09\n",
      "Episode 12, epsilon   0.01, batch size 24000, ep return 1.710386e+10\n",
      "Episode 13, epsilon   0.01, batch size 26000, ep return 1.909099e+10\n",
      "Episode 14, epsilon   0.01, batch size 28000, ep return 1.872254e+10\n",
      "Episode 15, epsilon   0.01, batch size 30000, ep return 2.010351e+10\n",
      "Episode 16, epsilon   0.01, batch size 32000, ep return 2.575420e+10\n",
      "Episode 17, epsilon   0.01, batch size 34000, ep return 2.533543e+10\n",
      "Episode 18, epsilon   0.01, batch size 36000, ep return 2.416658e+10\n",
      "Episode 19, epsilon   0.01, batch size 38000, ep return 2.882324e+10\n",
      "Episode 20, epsilon   0.01, batch size 40000, ep return 2.874956e+10\n",
      "Episode 21, epsilon   0.01, batch size 42000, ep return 2.743884e+10\n",
      "Episode 22, epsilon   0.01, batch size 44000, ep return 2.833622e+10\n",
      "Episode 23, epsilon   0.01, batch size 46000, ep return 2.750315e+10\n",
      "Episode 24, epsilon   0.01, batch size 48000, ep return 3.131247e+10\n",
      "Episode 25, epsilon   0.01, batch size 50000, ep return 3.125446e+10\n",
      "Episode 26, epsilon   0.01, batch size 52000, ep return 2.631397e+10\n",
      "Episode 27, epsilon   0.01, batch size 54000, ep return 3.210970e+10\n",
      "Episode 28, epsilon   0.01, batch size 56000, ep return 2.970829e+10\n",
      "Episode 29, epsilon   0.01, batch size 58000, ep return 2.762641e+10\n",
      "Episode 30, epsilon   0.01, batch size 60000, ep return 2.422008e+10\n",
      "Episode 31, epsilon   0.01, batch size 62000, ep return 2.906578e+10\n",
      "Episode 32, epsilon   0.01, batch size 64000, ep return 3.597606e+10\n",
      "Episode 33, epsilon   0.01, batch size 66000, ep return 3.348952e+10\n",
      "Episode 34, epsilon   0.01, batch size 68000, ep return 3.294805e+10\n",
      "Episode 35, epsilon   0.01, batch size 70000, ep return 3.057577e+10\n",
      "Episode 36, epsilon   0.01, batch size 72000, ep return 3.218919e+10\n",
      "Episode 37, epsilon   0.01, batch size 74000, ep return 2.938909e+10\n",
      "Episode 38, epsilon   0.01, batch size 76000, ep return 3.001167e+10\n",
      "Episode 39, epsilon   0.01, batch size 78000, ep return 3.130474e+10\n",
      "Episode 40, epsilon   0.01, batch size 80000, ep return 2.836541e+10\n",
      "Episode 41, epsilon   0.01, batch size 82000, ep return 2.578535e+10\n",
      "Episode 42, epsilon   0.01, batch size 84000, ep return 2.922837e+10\n",
      "Episode 43, epsilon   0.01, batch size 86000, ep return 3.021150e+10\n",
      "Episode 44, epsilon   0.01, batch size 88000, ep return 2.647841e+10\n",
      "Episode 45, epsilon   0.01, batch size 90000, ep return 3.312353e+10\n",
      "Episode 46, epsilon   0.01, batch size 92000, ep return 2.865189e+10\n",
      "Episode 47, epsilon   0.01, batch size 94000, ep return 2.416016e+10\n",
      "Episode 48, epsilon   0.01, batch size 96000, ep return 3.102359e+10\n",
      "Episode 49, epsilon   0.01, batch size 98000, ep return 2.830600e+10\n",
      "Episode 50, epsilon   0.01, batch size 100000, ep return 2.979294e+10\n",
      "Episode 51, epsilon   0.01, batch size 102000, ep return 3.325308e+10\n",
      "Episode 52, epsilon   0.01, batch size 104000, ep return 3.080472e+10\n",
      "Episode 53, epsilon   0.01, batch size 106000, ep return 2.835725e+10\n",
      "Episode 54, epsilon   0.01, batch size 108000, ep return 3.127504e+10\n",
      "Episode 55, epsilon   0.01, batch size 110000, ep return 3.297747e+10\n",
      "Episode 56, epsilon   0.01, batch size 112000, ep return 3.373352e+10\n",
      "Episode 57, epsilon   0.01, batch size 114000, ep return 2.869516e+10\n",
      "Episode 58, epsilon   0.01, batch size 116000, ep return 3.365127e+10\n",
      "Episode 59, epsilon   0.01, batch size 118000, ep return 3.022298e+10\n",
      "Episode 60, epsilon   0.01, batch size 120000, ep return 3.447039e+10\n",
      "Episode 61, epsilon   0.01, batch size 122000, ep return 3.000535e+10\n",
      "Episode 62, epsilon   0.01, batch size 124000, ep return 3.021852e+10\n",
      "Episode 63, epsilon   0.01, batch size 126000, ep return 3.317568e+10\n",
      "Episode 64, epsilon   0.01, batch size 128000, ep return 2.937289e+10\n",
      "Episode 65, epsilon   0.01, batch size 130000, ep return 3.271721e+10\n",
      "Episode 66, epsilon   0.01, batch size 132000, ep return 3.018337e+10\n",
      "Episode 67, epsilon   0.01, batch size 134000, ep return 3.247996e+10\n",
      "Episode 68, epsilon   0.01, batch size 136000, ep return 2.675041e+10\n",
      "Episode 69, epsilon   0.01, batch size 138000, ep return 3.021550e+10\n",
      "Episode 70, epsilon   0.01, batch size 140000, ep return 2.715508e+10\n",
      "Episode 71, epsilon   0.01, batch size 142000, ep return 2.615051e+10\n",
      "Episode 72, epsilon   0.01, batch size 144000, ep return 2.094529e+10\n",
      "Episode 73, epsilon   0.01, batch size 146000, ep return 2.640644e+10\n",
      "Episode 74, epsilon   0.01, batch size 148000, ep return 2.743424e+10\n",
      "Episode 75, epsilon   0.01, batch size 150000, ep return 2.872789e+10\n",
      "Episode 76, epsilon   0.01, batch size 152000, ep return 3.066842e+10\n",
      "Episode 77, epsilon   0.01, batch size 154000, ep return 2.534084e+10\n",
      "Episode 78, epsilon   0.01, batch size 156000, ep return 2.659347e+10\n",
      "Episode 79, epsilon   0.01, batch size 158000, ep return 2.809138e+10\n",
      "Episode 80, epsilon   0.01, batch size 160000, ep return 2.915471e+10\n",
      "Episode 81, epsilon   0.01, batch size 162000, ep return 3.015242e+10\n",
      "Episode 82, epsilon   0.01, batch size 164000, ep return 2.891032e+10\n",
      "Episode 83, epsilon   0.01, batch size 166000, ep return 2.773475e+10\n",
      "Episode 84, epsilon   0.01, batch size 168000, ep return 3.035572e+10\n",
      "Episode 85, epsilon   0.01, batch size 170000, ep return 3.237222e+10\n",
      "Episode 86, epsilon   0.01, batch size 172000, ep return 2.667042e+10\n",
      "Episode 87, epsilon   0.01, batch size 174000, ep return 2.896553e+10\n",
      "Episode 88, epsilon   0.01, batch size 176000, ep return 3.167032e+10\n",
      "Episode 89, epsilon   0.01, batch size 178000, ep return 2.520323e+10\n",
      "Episode 90, epsilon   0.01, batch size 180000, ep return 3.125216e+10\n",
      "Episode 91, epsilon   0.01, batch size 182000, ep return 2.630455e+10\n",
      "Episode 92, epsilon   0.01, batch size 184000, ep return 2.952006e+10\n",
      "Episode 93, epsilon   0.01, batch size 186000, ep return 2.764410e+10\n",
      "Episode 94, epsilon   0.01, batch size 188000, ep return 2.817240e+10\n",
      "Episode 95, epsilon   0.01, batch size 190000, ep return 2.993860e+10\n",
      "Episode 96, epsilon   0.01, batch size 192000, ep return 3.465166e+10\n",
      "Episode 97, epsilon   0.01, batch size 194000, ep return 3.003842e+10\n",
      "Episode 98, epsilon   0.01, batch size 196000, ep return 3.027712e+10\n",
      "Episode 99, epsilon   0.01, batch size 198000, ep return 3.097294e+10\n",
      "Episode 100, epsilon   0.01, batch size 200000, ep return 2.975491e+10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([ 0.10188705, 15.02289084, 16.46054196, 12.96632743, 14.54404047,\n",
       "         17.77884244, 13.95651692, 17.66745335, 12.94324555,  0.02540836]),\n",
       "  array([0.04286502, 0.04246964, 0.04106352, 0.03657982, 0.04382233,\n",
       "         0.03134315, 0.05172452, 0.03427512, 0.04263521, 0.0394372 ]),\n",
       "  array([0.03328874, 0.04485917, 0.05224731, 0.05227478, 0.03595511,\n",
       "         0.03183526, 0.05573974, 0.0314396 , 0.04362751, 0.03754075]),\n",
       "  array([0.04369149, 0.0287353 , 0.04955694, 0.03788963, 0.02998705,\n",
       "         0.05483282, 0.04468754, 0.0390661 , 0.09194184, 0.04599451]),\n",
       "  array([0.03179743, 0.10094113, 0.07405675, 0.03704915, 0.02914553,\n",
       "         0.04202346, 0.0470771 , 0.03903903, 1.09976747, 0.04553697]),\n",
       "  array([ 3.17662018,  0.02275863,  3.88506405,  0.47947493,  0.02806161,\n",
       "          1.84099532,  0.02432202, 13.55482591,  3.08839573,  1.33733763]),\n",
       "  array([ 5.81993942, 18.92666927,  9.02143227, 19.96347955, 15.1725871 ,\n",
       "         15.33430705,  3.8538961 , 16.7976175 ,  2.7159298 , 11.42639576]),\n",
       "  array([37.92371635, 33.37735643,  8.99089865,  8.85299158, 56.75208938,\n",
       "         61.89020263, 19.77121815, 12.20431583, 45.5628884 , 38.10502698]),\n",
       "  array([55.99554688, 51.02702885, 59.67025008, 45.37974014, 63.85806235,\n",
       "         55.86280928, 71.62375196, 38.91183908, 64.26313194, 63.86662134]),\n",
       "  array([ 8.08160754,  9.31473882,  8.86206638,  9.57966782, 11.65900988,\n",
       "         10.1266635 ,  8.58586638,  9.70827754, 64.47431111,  7.74555301]),\n",
       "  array([ 8.72053111,  9.22640373, 40.80254444,  6.99121144,  9.10207834,\n",
       "          8.27827206, 45.96409301,  8.68896207, 48.13978019, 34.25969704]),\n",
       "  array([56.5694284 , 25.32693929, 32.11826437, 55.13082831, 62.088082  ,\n",
       "         57.46746847, 49.74246833, 29.0659136 , 66.90515659, 49.87510669]),\n",
       "  array([55.06316915, 61.98430233, 58.9823167 , 53.59025253, 67.84229969,\n",
       "          0.58129263, 70.58626742, 47.55746258, 54.93033185, 69.43665609]),\n",
       "  array([52.75600789, 53.60414537, 65.76342237, 40.32052921, 44.6497006 ,\n",
       "         31.33180807, 58.5331415 , 68.24107979, 66.19993094, 48.72214373]),\n",
       "  array([61.12185791, 72.21186236, 45.60631787, 51.63017021, 69.39803456,\n",
       "         65.10148227, 54.22287402, 65.93637796, 33.93757905, 50.05680403]),\n",
       "  array([85.63209796, 76.9388127 , 60.46397331, 60.29748695, 69.11188522,\n",
       "         87.15141527, 59.31772869, 75.3358198 , 71.97980816, 82.99122282]),\n",
       "  array([75.65522541,  5.32370429, 79.59498203, 92.62810646, 74.57995262,\n",
       "         73.94141332, 76.55775136, 78.65415015, 89.69488256, 70.73294611]),\n",
       "  array([74.92347188, 80.07770495, 76.90240982, 79.8531622 , 56.23234477,\n",
       "         56.78655455, 63.92755612, 71.53787591, 61.74743564, 62.27917076]),\n",
       "  array([77.73583441, 83.73642018, 75.56536633, 91.03370496, 96.53971048,\n",
       "         57.4688511 , 88.77084266, 77.01899373, 94.45366526, 73.79548107]),\n",
       "  array([ 88.27806822,  78.50409752,  62.92326942,  96.08115047,\n",
       "          98.67836814,  75.92635078,  55.30886874, 100.75230595,\n",
       "          87.05187851,  70.5283832 ]),\n",
       "  array([108.77290362,  97.87212581,   0.20130118,  72.89363523,\n",
       "          92.35427895, 108.89769856,  60.09845311,  76.7466183 ,\n",
       "          88.91057574,  70.17272273]),\n",
       "  array([100.9511492 ,  74.17504294,  86.35715681,  73.06456211,\n",
       "          69.03378023,  83.22993476,  72.65709337,  89.61071111,\n",
       "          75.23433449,  78.01541429]),\n",
       "  array([86.33654994, 84.87703986, 96.17895099, 72.31117531, 90.78969972,\n",
       "         82.66432326, 86.56855531, 84.0409942 ,  0.27298776, 94.70085548]),\n",
       "  array([ 79.21525297,  89.99557341,  87.33220383,  74.09717899,\n",
       "         106.90107711,  89.35351738,  79.6577037 ,  95.14745349,\n",
       "          78.92566254, 105.97486013]),\n",
       "  array([ 79.6495483 , 108.90828851,  82.65349302,  88.96030711,\n",
       "          93.10120219,  74.74334638,  88.99198578,  98.211858  ,\n",
       "          93.72038975,  76.01755145]),\n",
       "  array([69.10928557, 84.19673324, 69.63836139, 81.4803812 , 85.05587195,\n",
       "         63.47489116, 68.48230004, 76.16948399, 82.00861326, 65.4541042 ]),\n",
       "  array([105.40755856, 104.18353832, 104.67349523,  79.06357472,\n",
       "          87.38999744,  99.98799357,  83.20667274, 102.96941558,\n",
       "          45.58011871,  96.71131895]),\n",
       "  array([ 96.99853417, 108.39862499,   0.27175641,  84.17356825,\n",
       "          94.01697003, 108.93650268,  82.90596291,  76.81614002,\n",
       "          89.88062891,  98.77994467]),\n",
       "  array([ 85.05577959,  72.70494863,  87.0213699 ,  98.65986411,\n",
       "          75.15927082,  95.97317643,  50.83221343, 110.23064281,\n",
       "          88.07132004,  18.52246415]),\n",
       "  array([43.27331222, 63.46944578, 86.78662481, 70.00901426, 80.68511423,\n",
       "         53.07168648, 70.79073688, 74.00272599, 86.12353711, 57.57010383]),\n",
       "  array([ 75.16142644,  71.25234387, 101.3547703 ,  92.26838863,\n",
       "          82.92677689,  98.31391452,  85.59123412,  83.36474299,\n",
       "          98.90657087,  33.84623979]),\n",
       "  array([ 99.76097319,  94.16282402, 103.50847037, 109.92570495,\n",
       "         107.73118379, 106.3882926 ,  90.49162745, 109.79293844,\n",
       "         106.68249589,  90.20344468]),\n",
       "  array([107.02659392,  80.4124685 ,  90.49666764, 106.26917531,\n",
       "          94.22442668, 104.57954002,  86.47141504,  86.01628365,\n",
       "         100.00228583,  92.74381299]),\n",
       "  array([104.26529967,  94.5452487 ,  87.44166308, 102.32594484,\n",
       "          77.60596236,  98.36247149, 102.89936093,  77.59437681,\n",
       "          89.90991476,  97.96079333]),\n",
       "  array([ 73.44008943, 104.81154999,  94.62238416,  95.38030897,\n",
       "          97.30493768,  65.36379389,  89.38292252,  75.44825734,\n",
       "          88.37859266,  81.60820427]),\n",
       "  array([ 88.66044524,  55.61225381,  99.5479972 ,  95.14319171,\n",
       "         107.4891446 ,  97.13582134,  95.39804436,  73.5333945 ,\n",
       "          99.86781356,  99.03637663]),\n",
       "  array([ 82.8392906 , 101.32505102,  93.48055   ,  30.58503745,\n",
       "          85.66192465,  93.28045239,  57.53204379, 105.23733989,\n",
       "          99.44577561,  82.75327363]),\n",
       "  array([ 71.4219099 , 107.16738993,  51.18739887,  93.33477112,\n",
       "         108.47363274,  74.33529797,  96.19081322, 101.88587082,\n",
       "          69.58797207,  76.18385335]),\n",
       "  array([ 91.99345566,  98.74402919, 100.23236554,  77.89515514,\n",
       "          90.12687351, 108.38461281,  74.63720196,  69.7188139 ,\n",
       "          88.18141882,  86.46765717]),\n",
       "  array([ 90.48849889,  68.88097192,  64.74830725,  73.10477548,\n",
       "          81.46846897,  81.19109308,  88.65942538, 107.78888015,\n",
       "          72.64972458,  74.17552792]),\n",
       "  array([68.82995865, 80.80531667, 73.35703775, 83.38751558, 77.53413682,\n",
       "         66.1898368 , 87.33026083, 49.09486584, 76.24960414, 67.32392248]),\n",
       "  array([ 82.04197659,  79.7812523 ,  77.7217346 ,  85.94632343,\n",
       "          77.26415432,  81.31359053,  76.58139837, 100.80885114,\n",
       "          90.36677216,  75.76396887]),\n",
       "  array([90.94615896, 70.47346096, 97.04871183, 83.17929845, 98.4438574 ,\n",
       "         86.17234799, 65.72041446, 87.33094258, 83.26772836, 92.84411617]),\n",
       "  array([61.31615265, 76.69853408, 68.7840824 , 74.56703073, 95.72821071,\n",
       "         85.63019795, 65.66221285, 76.48512578, 74.47866357, 70.37574111]),\n",
       "  array([ 97.68334087,  99.10177083, 110.51547045,  88.40693244,\n",
       "          94.33727016,  68.73279805,  86.60001042,  97.66972944,\n",
       "          84.49111351, 110.34131567]),\n",
       "  array([92.80438002, 67.15141396, 91.40400151, 95.33109858, 75.47050179,\n",
       "         63.57003002, 66.69851537, 86.14253929, 92.96742428, 79.7271957 ]),\n",
       "  array([57.36909312, 79.60162232, 53.54390305, 62.93765496, 69.23489588,\n",
       "         70.81586095, 73.97492026, 73.93826251, 69.03945593, 73.63024025]),\n",
       "  array([ 93.86089366,  88.48370979,  75.85155658, 106.85458487,\n",
       "         105.05618514,  92.03905901,  73.50935852,  92.98888888,\n",
       "          78.52217792,  71.25435378]),\n",
       "  array([85.87010662, 86.32417749, 75.12977123, 90.19361925, 66.3394093 ,\n",
       "         75.11668728, 75.0165347 , 73.03494222, 94.34053687, 80.1078234 ]),\n",
       "  array([75.59189101, 73.76812077, 90.22345891, 71.13554175, 97.50358907,\n",
       "         79.71322918, 74.94229286, 94.28835423, 94.89004923, 91.51904442]),\n",
       "  array([ 97.3682585 ,  81.00156306, 103.56784713, 102.95497788,\n",
       "          74.18765388, 106.13948587, 105.70516872,  77.20604517,\n",
       "          84.21962738, 109.19722472]),\n",
       "  array([107.78862338,  69.91987508,  79.31060038,  89.68837081,\n",
       "          89.02362633, 103.7758986 ,  77.78888918,  95.44713931,\n",
       "          57.2903888 , 102.19021995]),\n",
       "  array([87.43766525, 91.06149873, 73.78721677, 78.06253178, 92.50516117,\n",
       "         96.0332672 , 51.05078935, 82.53088678, 77.25644531, 73.19919309]),\n",
       "  array([ 80.12770556,  83.7394377 ,  93.77429605,  93.35975906,\n",
       "          96.75386894,  94.24456659,  41.85565775,  91.175188  ,\n",
       "         109.66249123, 100.84768338]),\n",
       "  array([ 98.79146395, 104.08685779,  99.10998314,  97.96825068,\n",
       "          78.81171135,  74.33226685, 105.90555753, 102.42923488,\n",
       "          79.65161327,  92.65721822]),\n",
       "  array([112.19722665,  78.53512802,  95.45814953, 102.77269755,\n",
       "          94.09977023,  96.24336521, 103.43605346,  89.89513812,\n",
       "          92.4019679 ,  90.11196428]),\n",
       "  array([ 78.65175214, 101.33975888,  72.07109398,  79.28548488,\n",
       "          36.37316916, 102.0614254 ,  73.72152068,  81.06762473,\n",
       "         106.76621584,  81.15448103]),\n",
       "  array([ 92.68086135, 105.86373077,  94.8083192 ,  82.80521921,\n",
       "          90.65149425, 111.41960098,  99.31955781,  98.07265729,\n",
       "         100.95628294,  76.24475144]),\n",
       "  array([ 85.60055715,  73.66534501,  82.21847981,  78.65411567,\n",
       "         106.99892089,  98.16532593,  83.98127922,  73.41226504,\n",
       "          76.89351277,  96.16210923]),\n",
       "  array([ 89.7142437 , 111.82602622,  90.49616198,  74.84055922,\n",
       "         102.7829984 ,  97.00911868, 105.48353718, 101.89366585,\n",
       "          96.23432091, 105.73492556]),\n",
       "  array([65.58293957, 95.29317841, 87.87373728, 87.79723718, 72.80056052,\n",
       "         89.0157995 , 85.88544481, 91.99057985, 77.54915614, 95.80134868]),\n",
       "  array([ 83.28861208, 103.50063902,  79.38843357,  95.55780365,\n",
       "         103.78048684,  94.45658016,  64.33379236,  31.42963975,\n",
       "          97.31772369, 102.57193456]),\n",
       "  array([105.50651291,  94.49275343,  94.09911889, 100.72253471,\n",
       "         107.87941675,  86.73722707,  93.51883292, 106.34027984,\n",
       "          71.23471662,  78.8248909 ]),\n",
       "  array([96.96824136, 91.04585345, 74.14442995, 41.30506249, 91.7563873 ,\n",
       "         83.00582347, 92.41401852, 83.54559947, 79.77249241, 97.72408004]),\n",
       "  array([102.63219756,  99.87248231,  88.06420736,  77.65733388,\n",
       "          97.97989957,  87.95925497,  96.23424924,  83.29810493,\n",
       "          94.11485683,  98.56255334]),\n",
       "  array([64.93758555, 90.11617742, 90.46213423, 96.93674936, 67.42865585,\n",
       "         83.97804816, 91.2375048 , 95.66842426, 97.80021442, 76.06503772]),\n",
       "  array([102.24116271, 106.61708142, 100.68678092,  55.800618  ,\n",
       "          78.86138767,  91.86144476,  97.91909116,  75.23046566,\n",
       "         102.73205238, 107.70726904]),\n",
       "  array([ 93.54875065, 103.53041952,  82.48938676,  71.99589079,\n",
       "          43.42797146,  70.22455164,  64.74671945,  74.97821851,\n",
       "          66.46174211,  86.02406262]),\n",
       "  array([ 77.19650635, 115.02662061,  93.40121345,  86.51325158,\n",
       "          86.54227526, 102.769937  ,  72.3957187 ,  59.67321133,\n",
       "          81.43927146,  80.58214318]),\n",
       "  array([51.60657508, 78.03516186, 70.7186537 , 85.9008002 , 69.56017742,\n",
       "         73.62355777, 90.88950101, 87.78386426, 90.02622352, 70.74107484]),\n",
       "  array([66.89793415, 68.96072723, 91.50356546, 84.08564422, 91.38510561,\n",
       "         78.67544891, 65.93341137, 68.21857109, 88.51189085, 36.26941905]),\n",
       "  array([ 42.50752105,  83.71474528,  39.36942375,  61.02662191,\n",
       "          90.35428309,  74.0655675 ,  48.13565461, 100.98411532,\n",
       "          29.76633924,  23.13391186]),\n",
       "  array([78.72167784, 93.12495512, 78.75394774, 82.22406456, 35.13670947,\n",
       "         76.25395429, 94.13525755, 94.43279429, 35.45936304, 79.44556825]),\n",
       "  array([100.95404353,  88.50188847,  85.28658686,  79.97610781,\n",
       "          69.10610951,  61.90637037,  64.97207143,  68.53399861,\n",
       "          87.73878673,  69.81412832]),\n",
       "  array([ 80.29975601,  57.33424019, 109.92818986, 100.97137147,\n",
       "          86.06546142, 110.15922856,  47.81252936,  63.70168853,\n",
       "          71.44779357,  85.69875525]),\n",
       "  array([67.75384465, 68.23141464, 99.88295409, 90.01090854, 96.75817519,\n",
       "         90.63124345, 91.13312468, 85.43988674, 81.28653378, 97.23628999]),\n",
       "  array([69.39380148, 74.65350078, 83.24606694, 78.07670046, 70.49082483,\n",
       "         49.17711069, 67.37060404, 85.01614307, 74.41591998, 65.67564386]),\n",
       "  array([75.26085873, 84.54864656, 82.72047135, 80.29909666, 51.95541938,\n",
       "         75.57305651, 84.06022882, 67.72958844, 86.09649097, 64.74010991]),\n",
       "  array([87.93087529, 93.17004322, 79.97090642, 75.94398206, 83.36062995,\n",
       "         89.27319902, 60.00692935, 70.1635483 , 85.52590303, 70.05069295]),\n",
       "  array([ 75.29847466,  73.68498989, 101.16792292,  92.01023423,\n",
       "          81.51675737,  76.90303918,  85.0868879 ,  71.2565069 ,\n",
       "          86.52387115,  82.05560608]),\n",
       "  array([ 97.79895474,  88.19789341,  34.31434645, 100.20085678,\n",
       "          92.09717244,  77.12803378,  92.94433759,  86.66348179,\n",
       "          83.21937511, 101.18971372]),\n",
       "  array([ 88.56483884,  93.32806143,  92.13650853, 102.21774048,\n",
       "          82.53543046,  68.2480017 ,  74.41081492,  79.73380648,\n",
       "          93.51569103,  43.89365405]),\n",
       "  array([ 44.89269549,  85.74393579,  43.57854067,  90.24381646,\n",
       "         106.08083437,  87.92110831,  81.47452466, 101.94815922,\n",
       "          90.78539827,  52.62968727]),\n",
       "  array([100.47498459,  70.47614706, 114.01014203,  41.21264828,\n",
       "         104.65762977,  97.74399827,  48.76464296,  96.49283508,\n",
       "         113.54934698,  72.12818722]),\n",
       "  array([ 51.92818468,  96.36316407,  96.9644716 , 101.83101934,\n",
       "          95.55730194,  80.25634058,  94.22877349, 110.52843287,\n",
       "          92.04902539,  96.89995727]),\n",
       "  array([74.92350712, 84.79552612, 92.47551231, 85.38916954, 47.95295809,\n",
       "         49.85043103, 63.12689357, 78.54806153, 88.69743775, 89.40326925]),\n",
       "  array([75.01296659, 95.29013823, 83.21087598, 86.27859971, 96.41213356,\n",
       "         67.38458491, 95.87742639, 97.55825548, 85.96409552, 37.15879986]),\n",
       "  array([ 88.52071734,  94.23574531,  99.5345169 ,  78.64455812,\n",
       "          80.73051604,  82.10941477,  78.7455723 ,  96.01882414,\n",
       "         106.79678107,  91.39623578]),\n",
       "  array([60.64102225, 86.72280205, 67.56794485, 50.04599412, 92.05324261,\n",
       "         68.53780835, 55.74255473, 83.95713552, 59.68848749, 88.66285237]),\n",
       "  array([102.8477479 ,  85.56757069,  75.93513532,  86.94705709,\n",
       "         105.89881978,  85.63226279, 103.48669587,  64.14971996,\n",
       "          73.58716278, 100.84063325]),\n",
       "  array([100.0280713 ,  96.08865719,  83.3109477 ,  77.50437063,\n",
       "          55.0383833 ,  50.53878207,  67.00215481,  50.8422879 ,\n",
       "          92.96628445,  71.48337293]),\n",
       "  array([83.33827473, 84.18253929, 88.02441634, 68.86070705, 84.69317858,\n",
       "         84.38409236, 86.63692691, 85.77750159, 74.25332734, 95.69799208]),\n",
       "  array([ 81.99561559,  78.74887244,  82.42533057, 108.52318911,\n",
       "          65.4627218 ,  76.1936655 ,  72.94325519,  66.60241116,\n",
       "          67.0136704 ,  82.8234257 ]),\n",
       "  array([ 63.31974554,  88.76442352,  81.24883795,  75.17176122,\n",
       "          94.08063329,  68.40460726,  82.39048607,  80.30812324,\n",
       "         100.69108157,  63.31083104]),\n",
       "  array([108.30796679,  97.46977091,  52.23332401,  95.03857655,\n",
       "          99.33997118,  99.70801365,  72.37584852,  96.75111926,\n",
       "          76.84219126,  49.63302678]),\n",
       "  array([106.70817661, 109.52789237, 104.01072885,  73.40787531,\n",
       "          91.13502444, 105.20342178,  99.99736327,  88.27421104,\n",
       "         103.64422615,  99.23922545]),\n",
       "  array([ 74.12253851,  75.1299268 ,  89.91358145,  71.02139483,\n",
       "         102.12994149,  91.41931563,  87.72541744,  92.71716869,\n",
       "          94.30663179,  72.04026814]),\n",
       "  array([ 87.64365681,  85.49294813,  97.25728296,  89.05368554,\n",
       "         103.95243375,  70.14949493,  88.98502638,  53.56453519,\n",
       "          88.30513903,  92.88083734]),\n",
       "  array([ 98.66465581, 100.14859392,  77.99008249,  92.78992363,\n",
       "         106.80268002,  91.69095081,  93.31825989,  76.29112295,\n",
       "          92.70071732,  46.5899138 ]),\n",
       "  array([ 86.44507094,  97.59331933,  95.68656662,  57.65786457,\n",
       "          88.59726494,  73.04673124,  74.89186166, 101.62813476,\n",
       "          82.39427767,  84.55777897])],\n",
       " [],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import env_hiv\n",
    "from typing import Protocol\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Declare network\n",
    "from env_hiv import HIVPatient\n",
    "env = HIVPatient(domain_randomization=True)\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from gymnasium.wrappers import TransformReward\n",
    "def rew(state,action, env) : \n",
    "    return-(\n",
    "                env.Q * state[4]\n",
    "                + env.R1 * action[0] ** 2\n",
    "                + env.R2 * action[1] ** 2\n",
    "                - env.S * state[5]\n",
    "            )\n",
    "low_r, high_r = rew(env.lower, [1,1],env),rew(env.upper, [1,1],env)\n",
    "env = TransformReward(env, lambda r: (r-low_r)/(high_r-low_r))\n",
    "env = TimeLimit(env,200)\n",
    "env =  gym.vector.AsyncVectorEnv([lambda : env for i in range(10)])\n",
    "def descale(r) :\n",
    "    return r * (high_r - low_r) +low_r\n",
    "\n",
    "state_dim = env.observation_space.shape[1]\n",
    "n_action = env.action_space.nvec[0]\n",
    "nb_neurons=256\n",
    "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "\n",
    "config = {'nb_actions': n_action,\n",
    "          'learning_rate': 5e-5,\n",
    "          'gamma': 1,\n",
    "          'buffer_size': 1000000,\n",
    "          'epsilon_min': 0.01,\n",
    "          'epsilon_max': 1.,\n",
    "          'epsilon_decay_period': 1000,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'batch_size': 1024,\n",
    "          'gradient_steps':20,\n",
    "          'update_target_strategy': 'replace', # or 'ema'\n",
    "          'update_target_freq': 50,\n",
    "          'update_target_tau': 0.005,\n",
    "          'criterion': torch.nn.SmoothL1Loss(),\n",
    "          'monitoring_nb_trials': 0}\n",
    "\n",
    "from train_cma import ProjectAgent\n",
    "pi = ProjectAgent()\n",
    "pi.load()\n",
    "agent = dqn_agent(config, DQN,pi=pi)\n",
    "agent.train(env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    state= np.log(state+1e-9)\n",
    "    if len(torch.Tensor(state).shape)==1 :\n",
    "        state = torch.Tensor(state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).to(device))\n",
    "        return torch.argmax(Q, dim=1).cpu()\n",
    "class ProjectAgent:\n",
    "\n",
    "    def __init__(self) :\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env =env_hiv.HIVPatient(domain_randomization=False)\n",
    "        self.config = config\n",
    "        self.dqn = None\n",
    "    def act(self, observation: np.ndarray, use_random: bool = False) -> int:\n",
    "        return greedy_action(self.dqn, observation)\n",
    "\n",
    "    def save(self, path=\"\"):\n",
    "        serialized= {\"dqn\":self.dqn.cpu(), \"config\":self.config}\n",
    "        with open('DQNAGENTS/saved3.pkl', 'wb') as f:  # open a text file\n",
    "            pickle.dump(serialized, f) # serialize the list\n",
    "    def load(self):\n",
    "        with open('DQNAGENTS/saved3.pkl', 'rb') as f:  # open a text file\n",
    "            saved = pickle.load( f) # serialize the list\n",
    "        self.dqn= saved[\"dqn\"].to(self.device)\n",
    "        try : \n",
    "            x,_ = self.env.reset()\n",
    "            self.act(x)\n",
    "        except : \n",
    "            raise Exception(\"Actor incompatible with environnement\")\n",
    "    \n",
    "    def train(self):\n",
    "        return self.agent.train(self.env,self.config['epochs'])\n",
    "Pagent = ProjectAgent()\n",
    "Pagent.dqn = agent.model\n",
    "Pagent.save()\n",
    "Pagent.load()\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.516853e+10 3.138168e+10\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=Pagent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=Pagent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")\n",
    "print(\"{:e}\".format(score_agent),\"{:e}\".format(score_agent_dr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "names = os.listdir(\"DQNAGENTS/\")\n",
    "models= []\n",
    "for name in names :\n",
    "    with open(\"DQNAGENTS/\"+name, 'rb') as f:  # open a text file\n",
    "        models.append(pickle.load( f)['dqn']) # serialize the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProjectAgent:\n",
    "    def __init__(self) :\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env =env_hiv.HIVPatient(domain_randomization=False)\n",
    "        self.config = config\n",
    "        self.models = []\n",
    "    def act(self, observation: np.ndarray, use_random: bool = False) -> int:\n",
    "        observation = (torch.Tensor(observation).to(torch.float32)+1e-9).log()\n",
    "        logits = [m(observation) for m in self.models[:3]]\n",
    "        #logits =[models[0](observation)]+[models[2](observation)]\n",
    "        logits =torch.softmax(torch.stack(logits,0),1).mean(0)\n",
    "        return torch.argmax(logits).item()\n",
    "\n",
    "    def save(self, path=\"\"):\n",
    "        serialized= {\"models\" :self.models}\n",
    "        with open(path+'models.pkl', 'wb') as f:  # open a text file\n",
    "            pickle.dump(serialized, f) # serialize the list\n",
    "    def load(self):\n",
    "        with open('models.pkl', 'rb') as f:  # open a text file\n",
    "            saved = pickle.load( f) # serialize the list\n",
    "        self.models= saved[\"models\"]\n",
    "        try : \n",
    "            x,_ = self.env.reset()\n",
    "            self.act(x)\n",
    "        except : \n",
    "            raise Exception(\"Actor incompatible with environnement\")\n",
    "agent = ProjectAgent()\n",
    "agent.models= models\n",
    "agent.save()\n",
    "agent.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.877342e+10 2.528770e+10\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")\n",
    "print(\"{:e}\".format(score_agent),\"{:e}\".format(score_agent_dr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved2.pkl', 'saved0.pkl', 'saved3.pkl', 'saved1.pkl']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
