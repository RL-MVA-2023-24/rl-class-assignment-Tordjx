{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_hiv import *\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import joblib\n",
    "def setstate(env,state):\n",
    "    env.T1 = state[0]\n",
    "    env.T1star = state[1]\n",
    "    env.T2 = state[2]\n",
    "    env.T2star = state[3]\n",
    "    env.V = state[4]\n",
    "    env.E = state[5]\n",
    "    return env\n",
    "def beamsearch(env,state,action,depth, maxdepth) :\n",
    "    env = setstate(env,state)\n",
    "    state2, r,_,_,_ = env.step(action)\n",
    "    possible_actions = [i for i in range(4)]\n",
    "    if maxdepth == depth :\n",
    "        return r\n",
    "\n",
    "    else: \n",
    "        return r+max([beamsearch(env,state2,a,depth+1, maxdepth) for a in possible_actions])\n",
    "def solve(env, maxdepth = 4) : \n",
    "    policy = []\n",
    "    cumreward = 0\n",
    "    state,_ = env.reset()\n",
    "    possible_actions = [i for i in range(4)]\n",
    "    for t in tqdm(range(200)):\n",
    "        beam = joblib.Parallel(n_jobs=os.cpu_count())(joblib.delayed(beamsearch)(env, state, a,0, maxdepth) for a in possible_actions)\n",
    "        action = np.argmax(beam)\n",
    "        state , reward,_,_,_= env.step(action)\n",
    "        cumreward+=reward\n",
    "        policy.append(action)\n",
    "    return policy,cumreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##VERSION 64 CPU\n",
    "\"\"\"def beamsearchinit(env,state,action,depth,maxdepth):\n",
    "    env = setstate(env,state)\n",
    "    state2, r1,_,_,_ = env.step(action[0])\n",
    "    state2, r2,_,_,_ = env.step(action[1])\n",
    "    return r1+r2+beamsearch(env,state2,action[2],depth+2, maxdepth)\n",
    "def solve(env, maxdepth = 4) : \n",
    "    policy = []\n",
    "    cumreward = 0\n",
    "    state,_ = env.reset()\n",
    "    possible_actions = [[i,j,k] for i in range(4) for j in range(4) for k in range(4)]\n",
    "    for t in tqdm(range(200)):\n",
    "        beam = joblib.Parallel(n_jobs=os.cpu_count())(joblib.delayed(beamsearchinit)(env, state, a,0, maxdepth) for a in possible_actions)\n",
    "        action = possible_actions[np.argmax(beam)][0]\n",
    "        state , reward,_,_,_= env.step(action)\n",
    "        cumreward+=reward\n",
    "        policy.append(action)\n",
    "    return policy,cumreward\"\"\"\n",
    "\n",
    "##VERSION 128 CPU\n",
    "\"\"\"def beamsearchinit(env,state,action,depth,maxdepth):\n",
    "    env = setstate(env,state)\n",
    "    state2, r1,_,_,_ = env.step(action[0])\n",
    "    state2, r2,_,_,_ = env.step(action[1])\n",
    "    state2, r3,_,_,_ = env.step(action[2])\n",
    "    return r1+r2+r3+beamsearch(env,state2,action[2],depth+3, maxdepth)\n",
    "def solve(env, maxdepth = 4) : \n",
    "    policy = []\n",
    "    cumreward = 0\n",
    "    state,_ = env.reset()\n",
    "    possible_actions = [[i,j,k,l] for i in range(4) for j in range(4) for k in range(4) for l in range(4)]\n",
    "    for t in tqdm(range(200)):\n",
    "        beam = joblib.Parallel(n_jobs=os.cpu_count())(joblib.delayed(beamsearchinit)(env, state, a,0, maxdepth) for a in possible_actions)\n",
    "        action = possible_actions[np.argmax(beam)][0]\n",
    "        state , reward,_,_,_= env.step(action)\n",
    "        cumreward+=reward\n",
    "        policy.append(action)\n",
    "    return policy,cumreward\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_hiv import *\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import joblib\n",
    "def setstate(env,state):\n",
    "    env.T1 = state[0]\n",
    "    env.T1star = state[1]\n",
    "    env.T2 = state[2]\n",
    "    env.T2star = state[3]\n",
    "    env.V = state[4]\n",
    "    env.E = state[5]\n",
    "    return env\n",
    "def beamsearch(env,state,action,depth, maxdepth) :\n",
    "    env = setstate(env,state)\n",
    "    state2, r,_,_,_ = env.step(action)\n",
    "    possible_actions = [i for i in range(4)]\n",
    "    if maxdepth == depth :\n",
    "        return r\n",
    "\n",
    "    else: \n",
    "        return r+max([beamsearch(env,state2,a,depth+1, maxdepth) for a in possible_actions])\n",
    "\n",
    "def beamsearchinit(env,state,action,depth,maxdepth):\n",
    "    env = setstate(env,state)\n",
    "    state2, r,_,_,_ = env.step(action[0])\n",
    "    return r+beamsearch(env,state2,action[1],depth+1, maxdepth)\n",
    "def solve(env, maxdepth = 4) : \n",
    "    policy = []\n",
    "    cumreward = 0\n",
    "    state,_ = env.reset()\n",
    "    possible_actions = [[i,j] for i in range(4) for j in range(4)]\n",
    "    for t in tqdm(range(200)):\n",
    "        beam = joblib.Parallel(n_jobs=os.cpu_count())(joblib.delayed(beamsearchinit)(env, state, a,0, maxdepth) for a in possible_actions)\n",
    "        action = possible_actions[np.argmax(beam)][0]\n",
    "        setstate(env,state)\n",
    "        state , reward,_,_,_= env.step(action)\n",
    "        cumreward+=reward\n",
    "        policy.append(action)\n",
    "    return policy,cumreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [08:31<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "env = HIVPatient(domain_randomization=True)\n",
    "pi,reward = solve(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.052717e+10'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:e}\".format(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectAgent : \n",
    "    def __init__(self,pi) -> None:\n",
    "        self.pi = pi\n",
    "        self.cont = 0\n",
    "    def act(self, obs, use_random =False) : \n",
    "        \n",
    "        self.cont+=1\n",
    "        return self.pi[self.cont-1]\n",
    "agent = ProjectAgent(pi)\n",
    "import random , torch , os\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "from evaluate import *\n",
    "seed_everything(seed=42)\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "print('indiv ', {\"{:e}\".format(score_agent)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
