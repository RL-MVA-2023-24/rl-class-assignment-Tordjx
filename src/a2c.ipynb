{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from train_a2c import *\n",
    "# from env_hiv import *\n",
    "# #env = HIVPatient(domain_randomization=False)\n",
    "# env = gym.make('Acrobot', render_mode=\"rgb_array\")\n",
    "# config = {'gamma': .99,\n",
    "#           'learning_rate': 3e-3,\n",
    "#           'nb_episodes': 10,\n",
    "#           'entropy_coefficient': 1e-1\n",
    "#          }\n",
    "\n",
    "# pi = policyNetwork(env)\n",
    "# value  = valueNetwork(env)\n",
    "# agent = a2c_agent(config, pi, value, maxiter = 200)\n",
    "# res = agent.train(env, 50)\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [01:30<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 131\u001b[0m\n\u001b[1;32m    125\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m.99\u001b[39m,\n\u001b[1;32m    126\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m    127\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnb_episodes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m    128\u001b[0m          }\n\u001b[1;32m    130\u001b[0m agent \u001b[38;5;241m=\u001b[39m ProjectAgent(config,env)\n\u001b[0;32m--> 131\u001b[0m returns\u001b[38;5;241m=\u001b[39m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m    133\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(returns)\n",
      "Cell \u001b[0;32mIn[5], line 113\u001b[0m, in \u001b[0;36mProjectAgent.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m,epochs):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 101\u001b[0m, in \u001b[0;36mreinforce_agent.train\u001b[0;34m(self, env, nb_rollouts)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trange(nb_rollouts) \u001b[38;5;28;01mas\u001b[39;00m pbar : \n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m--> 101\u001b[0m         avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix(avgreturn \u001b[38;5;241m=\u001b[39m avg)\n\u001b[1;32m    103\u001b[0m         avg_sum_rewards\u001b[38;5;241m.\u001b[39mappend(avg)\n",
      "Cell \u001b[0;32mIn[5], line 70\u001b[0m, in \u001b[0;36mreinforce_agent.one_gradient_step\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     69\u001b[0m     a, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_action_and_log_prob(x)\n\u001b[0;32m---> 70\u001b[0m     y,r,d,t,_ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     d \u001b[38;5;241m=\u001b[39m d \u001b[38;5;129;01mor\u001b[39;00m t\n\u001b[1;32m     72\u001b[0m     log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/rl-class-assignment-Tordjx/src/env_hiv.py:232\u001b[0m, in \u001b[0;36mHIVPatient.step\u001b[0;34m(self, a_index)\u001b[0m\n\u001b[1;32m    230\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate()\n\u001b[1;32m    231\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_set[a_index]\n\u001b[0;32m--> 232\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(state, action, state2)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipping:\n",
      "File \u001b[0;32m~/rl-class-assignment-Tordjx/src/env_hiv.py:213\u001b[0m, in \u001b[0;36mHIVPatient.transition\u001b[0;34m(self, state, action, duration)\u001b[0m\n\u001b[1;32m    211\u001b[0m nb_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_steps):\n\u001b[0;32m--> 213\u001b[0m     der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m state0 \u001b[38;5;241m+\u001b[39m der \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# np.clip(state1, self.lower, self.upper, out=state1)\u001b[39;00m\n",
      "File \u001b[0;32m~/rl-class-assignment-Tordjx/src/env_hiv.py:182\u001b[0m, in \u001b[0;36mHIVPatient.der\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    177\u001b[0m eps2 \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    179\u001b[0m T1dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md1 \u001b[38;5;241m*\u001b[39m T1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps1) \u001b[38;5;241m*\u001b[39m V \u001b[38;5;241m*\u001b[39m T1\n\u001b[1;32m    181\u001b[0m T1stardot \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps1) \u001b[38;5;241m*\u001b[39m V \u001b[38;5;241m*\u001b[39m T1 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m \u001b[38;5;241m*\u001b[39m T1star \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm1 \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m*\u001b[39m T1star\n\u001b[1;32m    183\u001b[0m )\n\u001b[1;32m    184\u001b[0m T2dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md2 \u001b[38;5;241m*\u001b[39m T2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk2 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m*\u001b[39m eps1) \u001b[38;5;241m*\u001b[39m V \u001b[38;5;241m*\u001b[39m T2\n\u001b[1;32m    185\u001b[0m T2stardot \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk2 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m*\u001b[39m eps1) \u001b[38;5;241m*\u001b[39m V \u001b[38;5;241m*\u001b[39m T2\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m*\u001b[39m T2star\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm2 \u001b[38;5;241m*\u001b[39m E \u001b[38;5;241m*\u001b[39m T2star\n\u001b[1;32m    189\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hsize = 128\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "class policyNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        n_action = env.action_space.n\n",
    "        self.fc1 = layer_init(nn.Linear(state_dim, hsize))\n",
    "        self.fc2 = layer_init(nn.Linear(hsize, n_action))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        x = F.relu(self.fc1(x.to(torch.float32)))\n",
    "        action_scores = self.fc2(x)\n",
    "        return F.softmax(action_scores,dim=1)\n",
    "\n",
    "    def sample_action(self, x):\n",
    "        probabilities = self.forward(x)\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        return action_distribution.sample().item()\n",
    "\n",
    "    def log_prob(self, x, a):\n",
    "        probabilities = self.forward(x)\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        return action_distribution.log_prob(a)\n",
    "class reinforce_agent:\n",
    "    def __init__(self, config, policy_network):\n",
    "        self.device = \"cuda\" if next(policy_network.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(policy_network.parameters()).dtype\n",
    "        self.policy = policy_network\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.99\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters()),lr=lr)\n",
    "        self.nb_episodes = config['nb_episodes'] if 'nb_episodes' in config.keys() else 1\n",
    "\n",
    "    def sample_action_and_log_prob(self, x):\n",
    "        probabilities = self.policy(torch.as_tensor(x))\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        action = action_distribution.sample()\n",
    "        log_prob = action_distribution.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "    \n",
    "    def one_gradient_step(self, env):\n",
    "        # run trajectories until done\n",
    "        episodes_sum_of_rewards = []\n",
    "        log_probs = []\n",
    "        returns = []\n",
    "        for ep in range(self.nb_episodes):\n",
    "            x,_ = env.reset()\n",
    "            rewards = []\n",
    "            episode_cum_reward = 0\n",
    "            while(True):\n",
    "                a, log_prob = self.sample_action_and_log_prob(x)\n",
    "                y,r,d,t,_ = env.step(a)\n",
    "                d = d or t\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(r)\n",
    "                episode_cum_reward += r\n",
    "                x=y\n",
    "                if d :\n",
    "                    # compute returns-to-go\n",
    "                    new_returns = []\n",
    "                    G_t = 0\n",
    "                    for r in reversed(rewards):\n",
    "                        G_t = r + self.gamma * G_t\n",
    "                        new_returns.append(G_t)\n",
    "                    new_returns = list(reversed(new_returns))\n",
    "                    returns.extend(new_returns)\n",
    "                    episodes_sum_of_rewards.append(episode_cum_reward)\n",
    "                    break\n",
    "        # make loss\n",
    "        returns = torch.tensor(returns)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        loss = -(returns * log_probs).sum()\n",
    "        # gradient step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return np.mean(episodes_sum_of_rewards)\n",
    "\n",
    "    def train(self, env, nb_rollouts):\n",
    "        avg_sum_rewards = []\n",
    "        with trange(nb_rollouts) as pbar : \n",
    "            for ep in pbar:\n",
    "                avg = self.one_gradient_step(env)\n",
    "                pbar.set_postfix(avgreturn = avg)\n",
    "                avg_sum_rewards.append(avg)\n",
    "        return avg_sum_rewards\n",
    "    \n",
    "class ProjectAgent() :\n",
    "    def __init__(self, config, env) : \n",
    "        self.config = config\n",
    "        self.env = env\n",
    "        self.pi = policyNetwork(self.env)\n",
    "        self.agent =reinforce_agent(config,self.pi)\n",
    "    def train(self,epochs):\n",
    "        return self.agent.train(env,epochs)\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "#env = gym.make('Acrobot', render_mode=\"rgb_array\")\n",
    "#env = gym.make('CartPole', render_mode=\"rgb_array\")\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "import env_hiv\n",
    "env = env_hiv.HIVPatient(domain_randomization=False)\n",
    "#env = gym.make(\"Acrobot\")\n",
    "env = TimeLimit(env,200)\n",
    "config = {'gamma': .99,\n",
    "          'learning_rate': 0.01,\n",
    "          'nb_episodes': 25\n",
    "         }\n",
    "\n",
    "agent = ProjectAgent(config,env)\n",
    "returns=agent.train(200)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
