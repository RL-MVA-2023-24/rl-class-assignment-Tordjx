{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le plan : CMA MAE -> 3/9 points\n",
    "#reinforce en loadant le policy network \n",
    "a=9709249010.93793\n",
    "b=2610906993.0028586\n",
    "from train import *\n",
    "pi = ProjectAgent()\n",
    "pi.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from torch.distributions import Categorical\n",
    "class reinforce_agent:\n",
    "    def __init__(self, config, policy_network):\n",
    "        self.device = \"cuda\" if next(policy_network.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(policy_network.parameters()).dtype\n",
    "        self.policy = policy_network\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.99\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters()),lr=lr)\n",
    "\n",
    "    def sample_action_and_log_prob(self, x):\n",
    "        logits = self.policy(torch.as_tensor(x).to(torch.float32))\n",
    "        probabilities =F.softmax(logits,dim=1)\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        action = action_distribution.sample()\n",
    "        log_prob = action_distribution.log_prob(action)\n",
    "        entropy = action_distribution.entropy()\n",
    "        return action, log_prob,entropy\n",
    "    def one_gradient_step(self, env):\n",
    "        # run trajectories until done\n",
    "        log_probs = [[] for i in range(env.observation_space.shape[0])]\n",
    "        returns = []\n",
    "        x,_ = env.reset()\n",
    "        rewards = [[] for i in range(env.observation_space.shape[0])]\n",
    "        entropies = []\n",
    "        episode_cum_reward = 0\n",
    "        for i in range(200):\n",
    "            a, log_prob ,entropy= self.sample_action_and_log_prob(x)\n",
    "            y,r,d,trunc,infos = env.step(a.numpy())\n",
    "            entropies.append(entropy)\n",
    "            for i in range(env.observation_space.shape[0]): \n",
    "                log_probs[i].append(log_prob[i])\n",
    "                rewards[i].append(r[i])\n",
    "                episode_cum_reward += r\n",
    "            x=y\n",
    "        for i in range(len(rewards)):\n",
    "            # compute returns-to-go\n",
    "            new_returns = []\n",
    "            G_t = 0\n",
    "            for r in reversed(rewards[i]):\n",
    "                G_t = r + self.gamma * G_t\n",
    "                new_returns.append(G_t)\n",
    "            new_returns = list(reversed(new_returns))\n",
    "            returns.extend(new_returns)\n",
    "            \n",
    "        # make loss\n",
    "        entropy_loss = -100*torch.cat(entropies).mean()\n",
    "        returns = torch.tensor(returns)\n",
    "        log_probs = torch.cat([torch.stack(u) for u in log_probs])\n",
    "        loss = -(returns * log_probs).mean()+ entropy_loss.to(self.device)\n",
    "        # gradient step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return np.mean(episode_cum_reward)/env.observation_space.shape[0], entropy_loss.item(), loss.item()\n",
    "\n",
    "    def train(self, env, nb_rollouts):\n",
    "        avg_sum_rewards = []\n",
    "        pbar = trange(nb_rollouts)\n",
    "        for ep in pbar:\n",
    "            avg,ent_loss ,loss= self.one_gradient_step(env)\n",
    "            avg = descale(avg)\n",
    "            pbar.set_postfix(avg=avg,ent_loss = ent_loss,loss= loss)\n",
    "            avg_sum_rewards.append(avg)\n",
    "        return avg_sum_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import gymnasium as gym\\nimport matplotlib.pyplot as plt\\nfrom env_hiv import *\\nenv =gym.vector.AsyncVectorEnv([lambda : HIVPatient(domain_randomization = True) for i in range(30)])\\nconfig = {'gamma': 1,\\n          'learning_rate': 0.001\\n         }\\nfrom train import *\\npi = ProjectAgent()\\npi.load()\\n\\n#ON VA FAIRE DU KAGGLE / DES AGENTS ENTRAINES AVEC OU SANS RANDOM, PLEINS\\n#pi = policyNetwork(env)\\n\\nagent = reinforce_agent(config, pi)\\nreturns = agent.train(env,50)\\nplt.plot(returns)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from env_hiv import *\n",
    "env =gym.vector.AsyncVectorEnv([lambda : HIVPatient(domain_randomization = True) for i in range(30)])\n",
    "config = {'gamma': 1,\n",
    "          'learning_rate': 0.001\n",
    "         }\n",
    "from train import *\n",
    "pi = ProjectAgent()\n",
    "pi.load()\n",
    "\n",
    "#ON VA FAIRE DU KAGGLE / DES AGENTS ENTRAINES AVEC OU SANS RANDOM, PLEINS\n",
    "#pi = policyNetwork(env)\n",
    "\n",
    "agent = reinforce_agent(config, pi)\n",
    "returns = agent.train(env,50)\n",
    "plt.plot(returns)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HIVPatient()\n",
    "from gymnasium.wrappers import TransformReward\n",
    "def rew(state,action, env) : \n",
    "    return-(\n",
    "                env.Q * state[4]\n",
    "                + env.R1 * action[0] ** 2\n",
    "                + env.R2 * action[1] ** 2\n",
    "                - env.S * state[5]\n",
    "            )\n",
    "low_r, high_r = rew(env.lower, [1,1],env),rew(env.upper, [1,1],env)\n",
    "env = TransformReward(env, lambda r: (r-low_r)/(high_r-low_r))\n",
    "scaledHIV = lambda domain_randomization : TransformReward(HIVPatient(domain_randomization), lambda r: (r-low_r)/(high_r-low_r))\n",
    "def descale(r) :\n",
    "    return r * (high_r - low_r) +low_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [03:10<06:45, 11.92s/it, avg=2.73e+9, ent_loss=-2.67, loss=-2.41]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m pi\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     13\u001b[0m agent \u001b[38;5;241m=\u001b[39m reinforce_agent(config, pi)\n\u001b[0;32m---> 14\u001b[0m returns \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m serialized\u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m : pi\u001b[38;5;241m.\u001b[39mget_params()}\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKAGGLETIER/saved\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# open a text file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 66\u001b[0m, in \u001b[0;36mreinforce_agent.train\u001b[0;34m(self, env, nb_rollouts)\u001b[0m\n\u001b[1;32m     64\u001b[0m pbar \u001b[38;5;241m=\u001b[39m trange(nb_rollouts)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 66\u001b[0m     avg,ent_loss ,loss\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_gradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     avg \u001b[38;5;241m=\u001b[39m descale(avg)\n\u001b[1;32m     68\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix(avg\u001b[38;5;241m=\u001b[39mavg,ent_loss \u001b[38;5;241m=\u001b[39m ent_loss,loss\u001b[38;5;241m=\u001b[39m loss)\n",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m, in \u001b[0;36mreinforce_agent.one_gradient_step\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m     33\u001b[0m     a, log_prob ,entropy\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_action_and_log_prob(x)\n\u001b[0;32m---> 34\u001b[0m     y,r,d,trunc,infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     entropies\u001b[38;5;241m.\u001b[39mappend(entropy)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]): \n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/site-packages/gymnasium/vector/vector_env.py:204\u001b[0m, in \u001b[0;36mVectorEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take an action for each parallel environment.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    {}\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/site-packages/gymnasium/vector/async_vector_env.py:322\u001b[0m, in \u001b[0;36mAsyncVectorEnv.step_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    320\u001b[0m successes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent_pipes):\n\u001b[0;32m--> 322\u001b[0m     result, success \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     successes\u001b[38;5;241m.\u001b[39mappend(success)\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from env_hiv import *\n",
    "import pickle\n",
    "config = {'gamma': 1,\n",
    "          'learning_rate': 0.001\n",
    "         }\n",
    "from train import *\n",
    "for i in range(10) :\n",
    "    env =gym.vector.AsyncVectorEnv([lambda : scaledHIV(domain_randomization = i%2) for j in range(30)])\n",
    "    pi = ProjectAgent()\n",
    "    pi.load()\n",
    "    agent = reinforce_agent(config, pi)\n",
    "    returns = agent.train(env,50)\n",
    "    serialized= {\"params\" : pi.get_params()}\n",
    "    with open('KAGGLETIER/saved'+str(i)+\".pkl\", 'wb') as f:  # open a text file\n",
    "        pickle.dump(serialized, f) # serialize the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class Aggregator : \n",
    "    def __init__(self) : \n",
    "        model_names = os.listdir(\"KAGGLETIER\")\n",
    "        self.models = []\n",
    "        for name in model_names : \n",
    "            pi = ProjectAgent()\n",
    "            with open('KAGGLETIER/'+name, 'rb') as f:  # open a text file\n",
    "                d= pickle.load( f) # serialize the list\n",
    "            pi.set_params(d['params'])\n",
    "            self.models.append(pi)\n",
    "    def act(self, obs):\n",
    "        logits = [m(obs) for m in self.models]\n",
    "        actions = []\n",
    "        for l in logits : \n",
    "            probabilities = nn.Softmax(dim = 1)(l)\n",
    "            action_distribution = Categorical(probabilities)\n",
    "            actions.append(action_distribution.sample().item())\n",
    "        counts = np.bincount(actions)\n",
    "        return np.argmax(counts)\n",
    "\n",
    "\n",
    "agg = Aggregator()\n",
    "env = HIVPatient()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0.0000,  526.3597, 1091.3484, 1042.7209]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 1.0000e+00, 7.6098e-22]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[  0.0000,   0.0000, 438.5038,   0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0., 0., 1., 0.]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "agg = Aggregator()\n",
    "env = HIVPatient(domain_randomization =True)\n",
    "x,_ = env.reset()\n",
    "for i in range(2):\n",
    "    a = agg.act(x)\n",
    "    x,_,_,_,_ = env.step(a)\n",
    "    print([m(x) for m in agg.models][0])\n",
    "    print(nn.Softmax(dim = 1)([m(x) for m in agg.models][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m agent\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Keep the following lines to evaluate your agent unchanged.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m score_agent: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_HIV\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m score_agent_dr: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m evaluate_HIV_population(agent\u001b[38;5;241m=\u001b[39magent, nb_episode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/rl-class-assignment-Tordjx/src/evaluate.py:30\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[0;34m(agent, env, nb_episode)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated:\n\u001b[1;32m     29\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(obs)\n\u001b[0;32m---> 30\u001b[0m     obs, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     32\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n",
      "File \u001b[0;32m~/miniconda3/envs/mujoco_py/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/rl-class-assignment-Tordjx/src/env_hiv.py:232\u001b[0m, in \u001b[0;36mHIVPatient.step\u001b[0;34m(self, a_index)\u001b[0m\n\u001b[1;32m    230\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate()\n\u001b[1;32m    231\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_set[a_index]\n\u001b[0;32m--> 232\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(state, action, state2)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipping:\n",
      "File \u001b[0;32m~/rl-class-assignment-Tordjx/src/env_hiv.py:212\u001b[0m, in \u001b[0;36mHIVPatient.transition\u001b[0;34m(self, state, action, duration)\u001b[0m\n\u001b[1;32m    210\u001b[0m state0_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(state)\n\u001b[1;32m    211\u001b[0m nb_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(duration \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_steps):\n\u001b[1;32m    213\u001b[0m     der \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mder(state0, action)\n\u001b[1;32m    214\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m state0 \u001b[38;5;241m+\u001b[39m der \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population\n",
    "import random\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(seed=42)\n",
    "# Initialization of the agent. Replace DummyAgent with your custom agent implementation.\n",
    "agent = ProjectAgent()\n",
    "agent.load()\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9709249010.93793, 2610906993.0028586)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=9709249010.93793\n",
    "b=2610906993.0028586\n",
    "score_agent,score_agent_dr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
