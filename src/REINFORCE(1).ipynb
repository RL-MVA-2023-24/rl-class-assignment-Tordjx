{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##classes\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "hsize = 128\n",
    "class policyNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        state_dim = env.observation_space.shape[1]\n",
    "        n_action = env.action_space.nvec[0]\n",
    "        self.fc1 = layer_init(nn.Linear(state_dim, hsize)).to(self.device)\n",
    "        #self.fc2 = layer_init(nn.Linear(hsize, hsize)).to(self.device)\n",
    "        self.fc3 = layer_init(nn.Linear(hsize, n_action)).to(self.device)\n",
    "        self.dropout =nn.Dropout(p=0).to(self.device)\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        x = F.relu(self.fc1(x.to(self.device)))\n",
    "        x= self.dropout(x)\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        action_scores = self.fc3(x)\n",
    "        return F.softmax(action_scores,dim=1).cpu()\n",
    "\n",
    "    def sample_action(self, x):\n",
    "        probabilities = self.forward(x)\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        return action_distribution.sample().item()\n",
    "\n",
    "    def log_prob(self, x, a):\n",
    "        probabilities = self.forward(x)\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        return action_distribution.log_prob(a)\n",
    "class valueNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        state_dim = env.observation_space.shape[1]\n",
    "        self.fc1 = layer_init(nn.Linear(state_dim, hsize)).to(self.device)\n",
    "        #self.fc2 = layer_init(nn.Linear(hsize, hsize)).to(self.device)\n",
    "        self.fc3 = layer_init(nn.Linear(hsize, 1)).to(self.device)\n",
    "        self.dropout =nn.Dropout(p=0).to(self.device)\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        x = F.relu(self.fc1(x.to(self.device)))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x= self.dropout(x)\n",
    "        return self.fc3(x).cpu()\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "maxiter =200\n",
    "class a2c_agent:\n",
    "    def __init__(self, config, policy_network,value_network):\n",
    "        self.device = \"cuda\" if next(policy_network.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(policy_network.parameters()).dtype\n",
    "        self.policy = policy_network\n",
    "        self.value = value_network\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.99\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters()),lr=lr)\n",
    "        self.entropy_coefficient = config['entropy_coefficient'] if 'entropy_coefficient' in config.keys() else 0.001\n",
    "\n",
    "    def sample_action(self, x):\n",
    "        probabilities = self.policy(torch.as_tensor(x))\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        action = action_distribution.sample()\n",
    "        log_prob = action_distribution.log_prob(action)\n",
    "        entropy = action_distribution.entropy()\n",
    "        return action, log_prob, entropy\n",
    "    \n",
    "    def one_gradient_step(self, env):\n",
    "        # run trajectories until done\n",
    "        episodes_sum_of_rewards = []\n",
    "        log_probs = [[] for i in range(env.observation_space.shape[0])]\n",
    "        returns = []\n",
    "        x,_ = env.reset()\n",
    "        rewards = [[] for i in range(env.observation_space.shape[0])]\n",
    "        values = [[] for i in range(env.observation_space.shape[0])]\n",
    "        entropies = [[] for i in range(env.observation_space.shape[0])]\n",
    "        is_done = torch.zeros(env.observation_space.shape[0])\n",
    "        is_trunc = torch.zeros(env.observation_space.shape[0])\n",
    "        episode_cum_reward = 0\n",
    "        #iters= 0\n",
    "        while(True):\n",
    "            #iters+=1\n",
    "            a, log_prob,entropy = self.sample_action(x)\n",
    "            y,r,d,trunc,infos = env.step(a.numpy())\n",
    "            is_done += d\n",
    "            is_trunc += trunc\n",
    "            V=self.value(torch.from_numpy(x))\n",
    "            for i,d in enumerate(is_done): \n",
    "                if d ==0 :\n",
    "                    log_probs[i].append(log_prob[i])\n",
    "                    rewards[i].append(r[i])\n",
    "                    values[i].append(V[i])\n",
    "                    entropies[i].append(entropy[i])\n",
    "                    episode_cum_reward += r\n",
    "            x=y\n",
    "            if all(is_done>0) or all(is_trunc>0) :#or iters>maxiter:\n",
    "                for i in range(len(is_done)):\n",
    "                    # compute returns-to-go\n",
    "                    new_returns = []\n",
    "                    G_t = 0\n",
    "                    for r in reversed(rewards[i]):\n",
    "                        G_t = r + self.gamma * G_t\n",
    "                        new_returns.append(G_t)\n",
    "                    new_returns = list(reversed(new_returns))\n",
    "                    returns.extend(new_returns)\n",
    "                    episodes_sum_of_rewards.append(episode_cum_reward)\n",
    "                break\n",
    "        # make loss\n",
    "        returns = torch.Tensor(returns)\n",
    "        values = torch.cat([torch.stack(u) for u in values])\n",
    "        log_probs = torch.cat([torch.stack(u) for u in log_probs])\n",
    "        entropies = torch.cat([torch.stack(u) for u in entropies])\n",
    "        advantages = returns - values\n",
    "        pg_loss = -(advantages.detach() * log_probs).mean()\n",
    "        entropy_loss = -entropies.mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        loss = pg_loss + critic_loss + self.entropy_coefficient * entropy_loss\n",
    "        # gradient step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return np.mean(episodes_sum_of_rewards),loss,critic_loss, pg_loss\n",
    "\n",
    "    def train(self, env, nb_rollouts):\n",
    "        self.value = self.value.train()\n",
    "        writer = SummaryWriter(\"runs/lr1e-2-ortho-128-2layers-entropy-4-timelimit\")\n",
    "        self.policy = self.policy.train()\n",
    "        avg_sum_rewards = []\n",
    "        pbar = trange(nb_rollouts)\n",
    "        for ep in pbar:\n",
    "            avg ,loss,critic_loss, pg_loss= self.one_gradient_step(env)\n",
    "            avg = avg/env.observation_space.shape[0]\n",
    "            pbar.set_postfix(avg_return = avg)\n",
    "            avg_sum_rewards.append(avg)\n",
    "            writer.add_scalar(\"Average return\",avg , ep)\n",
    "            writer.add_scalar(\"Loss\",loss , ep)\n",
    "            writer.add_scalar(\"Critic loss\",critic_loss , ep)\n",
    "            writer.add_scalar(\"PG loss\",pg_loss , ep)\n",
    "        self.value = self.value.eval()\n",
    "        self.policy = self.policy.eval()\n",
    "        return avg_sum_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-17431f9a1f920e69\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-17431f9a1f920e69\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 82/1000 [10:05<1:52:47,  7.37s/it, avg_return=7.11e+6] "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "#envs = gym.make_vec(\"CartPole-v1\", num_envs=5)\n",
    "#envs = gym.make_vec(\"LunarLander-v2\", num_envs=20)\n",
    "import env_hiv\n",
    "envs = gym.vector.AsyncVectorEnv([lambda: TimeLimit(env_hiv.HIVPatient(domain_randomization=True),200) for i in range(20)])\n",
    "#envs = gym.vector.AsyncVectorEnv([lambda: gym.make(\"Acrobot\", render_mode=\"rgb_array\") for i in range(20)])\n",
    "config = {'gamma': .99,\n",
    "          'learning_rate': 0.01,\n",
    "          \"entropy_coefficient\":1e-4\n",
    "         }\n",
    "value = valueNetwork(envs)\n",
    "pi = policyNetwork(envs)\n",
    "agent = a2c_agent(config, pi,value)\n",
    "returns = agent.train(envs,1000)\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tordjx/miniconda3/envs/mujoco_py/lib/python3.8/site-packages/gymnasium/envs/registration.py:521: UserWarning: \u001b[33mWARN: Using the latest versioned environment `Acrobot-v1` instead of the unversioned environment `Acrobot`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/tordjx/rl-class-assignment-Tordjx/src/videos/reinforce_policy-episode-0.mp4.\n",
      "Moviepy - Writing video /home/tordjx/rl-class-assignment-Tordjx/src/videos/reinforce_policy-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/tordjx/rl-class-assignment-Tordjx/src/videos/reinforce_policy-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"videos/reinforce_policy-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "test_env = gym.make(\"Acrobot\", render_mode=\"rgb_array_list\")\n",
    "#test_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "s,_ = test_env.reset()\n",
    "with torch.no_grad():\n",
    "    for t in range(1000):\n",
    "        a = pi.sample_action(torch.as_tensor(s))\n",
    "        s2,r,d,trunc,_ = test_env.step(a)\n",
    "        s = s2\n",
    "        if d:\n",
    "            break\n",
    "\n",
    "save_video(test_env.render(), \"videos\", fps=test_env.metadata[\"render_fps\"], name_prefix=\"reinforce_policy\")\n",
    "from IPython.display import Video\n",
    "Video(\"videos/reinforce_policy-episode-0.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
