{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.063500e+10'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from env_hiv import * \n",
    "env =  HIVPatient()\n",
    "def rew(state,action, env) : \n",
    "    return-(\n",
    "                env.Q * state[4]\n",
    "                + env.R1 * action[0] ** 2\n",
    "                + env.R2 * action[1] ** 2\n",
    "                - env.S * state[5]\n",
    "            )\n",
    "low_r, high_r = rew(env.lower, [1,1],env),rew(env.upper, [0,0],env)\n",
    "\n",
    "\"{:e}\".format(200*high_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "try : \n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "except : \n",
    "    pass\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "config = {'nb_actions': 4,\n",
    "          'learning_rate': 5e-5,\n",
    "          'gamma': 1,\n",
    "          'buffer_size': 1000000,\n",
    "          'epsilon_min': 0.01,\n",
    "          'epsilon_max': 1.,\n",
    "          'epsilon_decay_period': 1000,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'batch_size': 1024,\n",
    "          'gradient_steps':20,\n",
    "          'update_target_strategy': 'ema', # or 'ema'\n",
    "          'update_target_freq': 50,\n",
    "          'update_target_tau': 0.9,\n",
    "          'criterion': torch.nn.SmoothL1Loss(),\n",
    "          'monitoring_nb_trials': 0}\n",
    "#NOISY\n",
    "\"\"\"config = {'nb_actions': n_action,\n",
    "          'learning_rate': 5e-5,\n",
    "          'gamma': 1,\n",
    "          'buffer_size': 500000,\n",
    "          'epsilon_min': 0,\n",
    "          'epsilon_max': 0.,\n",
    "          'epsilon_decay_period': 1000,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'batch_size': 2048,\n",
    "          'gradient_steps':3,\n",
    "          'update_target_strategy': 'replace', # or 'ema'\n",
    "          'update_target_freq': 50,\n",
    "          'update_target_tau': 0.005,\n",
    "          'criterion': torch.nn.SmoothL1Loss(),\n",
    "          'monitoring_nb_trials': 0}\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append(\"/home/tordjx/prioritized_experience_replay/\")\n",
    "from memory.buffer import  PrioritizedReplayBuffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    state= np.log(state+1e-9)\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).to(device))\n",
    "        return torch.argmax(Q, dim=1).cpu()\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "class dqn_agent:\n",
    "    def __init__(self, config, model,pi =None):\n",
    "        self.pi = pi\n",
    "        device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.device =device\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.memory  = buffer = PrioritizedReplayBuffer(6,1,buffer_size)\n",
    "        self.epsilon_max = config['epsilon_max'] if 'epsilon_max' in config.keys() else 1.\n",
    "        self.epsilon_min = config['epsilon_min'] if 'epsilon_min' in config.keys() else 0.01\n",
    "        self.epsilon_stop = config['epsilon_decay_period'] if 'epsilon_decay_period' in config.keys() else 1000\n",
    "        self.epsilon_delay = config['epsilon_delay_decay'] if 'epsilon_delay_decay' in config.keys() else 20\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.model = model \n",
    "        self.target_model = deepcopy(self.model).to(device)\n",
    "        self.criterion = config['criterion'] if 'criterion' in config.keys() else torch.nn.MSELoss()\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = config['optimizer'] if 'optimizer' in config.keys() else torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.nb_gradient_steps = config['gradient_steps'] if 'gradient_steps' in config.keys() else 1\n",
    "        self.update_target_strategy = config['update_target_strategy'] if 'update_target_strategy' in config.keys() else 'replace'\n",
    "        self.update_target_freq = config['update_target_freq'] if 'update_target_freq' in config.keys() else 20\n",
    "        self.update_target_tau = config['update_target_tau'] if 'update_target_tau' in config.keys() else 0.005\n",
    "        self.monitoring_nb_trials = config['monitoring_nb_trials'] if 'monitoring_nb_trials' in config.keys() else 0\n",
    "\n",
    "    def MC_eval(self, env, nb_trials):   # NEW NEW NEW\n",
    "        MC_total_reward = []\n",
    "        MC_discounted_reward = []\n",
    "        for _ in range(nb_trials):\n",
    "            x,_ = env.reset()\n",
    "            done = False\n",
    "            trunc = False\n",
    "            total_reward = 0\n",
    "            discounted_reward = 0\n",
    "            step = 0\n",
    "            while not (done or trunc):\n",
    "                a = greedy_action(self.model, x)\n",
    "                y,r,done,trunc,_ = env.step(a)\n",
    "                x = y\n",
    "                total_reward += r\n",
    "                discounted_reward += self.gamma**step * r\n",
    "                step += 1\n",
    "            MC_total_reward.append(total_reward)\n",
    "            MC_discounted_reward.append(discounted_reward)\n",
    "        return np.mean(MC_discounted_reward), np.mean(MC_total_reward)\n",
    "    \n",
    "    def V_initial_state(self, env, nb_trials):   # NEW NEW NEW\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nb_trials):\n",
    "                val = []\n",
    "                x,_ = env.reset()\n",
    "                val.append(self.model(torch.Tensor(x).unsqueeze(0).to(device)).max().item())\n",
    "        return np.mean(val)\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if self.memory.count > self.batch_size:\n",
    "            batch, weights ,idx = self.memory.sample(self.batch_size)\n",
    "            X, A, R, Y, D = batch\n",
    "            X, A, R, Y, D=X.to(self.device), A.to(self.device), R.to(self.device), Y.to(self.device), D.to(self.device)\n",
    "            X,Y = (X+1e-9).log() , (Y+1e-9).log()\n",
    "            action = torch.argmax(self.target_model(Y),1).to(self.device) #bsz \n",
    "            update = torch.addcmul(R, 1-D, self.model(Y).gather(1, action.to(torch.long).unsqueeze(1))[:,0], value=self.gamma).to(self.device)\n",
    "            \n",
    "            \"\"\"QYmax = self.target_model(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\"\"\"\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long)).to(self.device).squeeze(1)\n",
    "            if weights is None:\n",
    "                weights = torch.ones_like(QXA).to(self.device)\n",
    "            weights =weights.to(self.device)\n",
    "            loss = torch.mean((QXA- update)**2 * weights)\n",
    "            td_errors = torch.abs(QXA-update)\n",
    "            \n",
    "            self.memory.update_priorities(idx , td_errors.cpu().detach().numpy())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(),0.5)\n",
    "            self.optimizer.step() \n",
    "    \n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        MC_avg_total_reward = []   # NEW NEW NEW\n",
    "        MC_avg_discounted_reward = []   # NEW NEW NEW\n",
    "        V_init_state = []   # NEW NEW NEW\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            #if step > self.epsilon_delay:\n",
    "            #    epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "            # select epsilon-greedy action\n",
    "            if episode <11 and self.pi is not None:\n",
    "                action = self.pi.act(state)\n",
    "            else :\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                    \n",
    "                else:\n",
    "                    action = greedy_action(self.model, state)\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            for u in range(env.observation_space.shape[0]) :\n",
    "                self.memory.add((state[u], action[u], reward[u], next_state[u], done[u]))\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_model.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_model.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "                self.target_model.load_state_dict(target_state_dict)\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if any(done) or any(trunc):\n",
    "                episode += 1\n",
    "                \"\"\"if episode ==10  and self.pi is not None:\n",
    "                    for i in range(5000):\n",
    "                        self.gradient_step()\"\"\"\n",
    "                # Monitoring\n",
    "                if self.monitoring_nb_trials>0:\n",
    "                    MC_dr, MC_tr = self.MC_eval(env, self.monitoring_nb_trials)    # NEW NEW NEW\n",
    "                    V0 = self.V_initial_state(env, self.monitoring_nb_trials)   # NEW NEW NEW\n",
    "                    MC_avg_total_reward.append(MC_tr)   # NEW NEW NEW\n",
    "                    MC_avg_discounted_reward.append(MC_dr)   # NEW NEW NEW\n",
    "                    V_init_state.append(V0)   # NEW NEW NEW\n",
    "                    episode_return.append(episode_cum_reward)   # NEW NEW NEW\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(self.memory.count), \n",
    "                          \", ep return \", '{:e}'.format(episode_cum_reward), \n",
    "                          \", MC tot \", '{:6.2f}'.format(MC_tr),\n",
    "                          \", MC disc \", '{:6.2f}'.format(MC_dr),\n",
    "                          \", V0 \", '{:6.2f}'.format(V0),\n",
    "                          sep='')\n",
    "                else:\n",
    "                    episode_return.append(episode_cum_reward)\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(self.memory.count), \n",
    "                          \", ep return \", '{:e}'.format(descale(episode_cum_reward.mean())), \n",
    "                          sep='')\n",
    "\n",
    "                \n",
    "                state, _ = env.reset()\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "            if descale(np.mean(episode_cum_reward))>5e10 : \n",
    "                return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, V_init_state\n",
    "        return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, V_init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1, epsilon   1.00, batch size 2200, ep return 1.752890e+07\n",
      "Episode  2, epsilon   1.00, batch size 4400, ep return 1.801589e+07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m agent\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m dqn\n\u001b[1;32m     66\u001b[0m agent\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m dqn\n\u001b[0;32m---> 67\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 122\u001b[0m, in \u001b[0;36mdqn_agent.train\u001b[0;34m(self, env, max_episode)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_gradient_steps): \n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# update target network if needed\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 86\u001b[0m, in \u001b[0;36mdqn_agent.gradient_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((QXA\u001b[38;5;241m-\u001b[39m update)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m weights)\n\u001b[1;32m     84\u001b[0m td_errors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(QXA\u001b[38;5;241m-\u001b[39mupdate)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priorities\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtd_errors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     88\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/prioritized_experience_replay/memory/buffer.py:108\u001b[0m, in \u001b[0;36mPrioritizedReplayBuffer.update_priorities\u001b[0;34m(self, data_idxs, priorities)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_idx, priority \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_idxs, priorities):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# The first variant we consider is the direct, proportional prioritization where p_i = |δ_i| + eps,\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# where eps is a small positive constant that prevents the edge-case of transitions not being\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# revisited once their error is zero. (Section 3.3)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     priority \u001b[38;5;241m=\u001b[39m (priority \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_priority \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_priority, priority)\n",
      "File \u001b[0;32m~/prioritized_experience_replay/memory/tree.py:34\u001b[0m, in \u001b[0;36mSumTree.update\u001b[0;34m(self, data_idx, value)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m parent \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes[parent] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m change\n\u001b[0;32m---> 34\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import env_hiv\n",
    "from typing import Protocol\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Declare network\n",
    "from env_hiv import HIVPatient\n",
    "env = HIVPatient(domain_randomization=True)\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from gymnasium.wrappers import TransformReward\n",
    "def rew(state,action, env) : \n",
    "    return-(\n",
    "                env.Q * state[4]\n",
    "                + env.R1 * action[0] ** 2\n",
    "                + env.R2 * action[1] ** 2\n",
    "                - env.S * state[5]\n",
    "            )\n",
    "low_r, high_r = rew(env.lower, [1,1],env),rew(env.upper, [1,1],env)\n",
    "env = TransformReward(env, lambda r: (r-low_r)/(high_r-low_r))\n",
    "env = TimeLimit(env,200)\n",
    "env =  gym.vector.AsyncVectorEnv([lambda : env for i in range(11)])\n",
    "def descale(r) :\n",
    "    return r * (high_r - low_r) +low_r\n",
    "\n",
    "state_dim = env.observation_space.shape[1]\n",
    "n_action = env.action_space.nvec[0]\n",
    "nb_neurons=256\n",
    "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "with open('DQNAGENTS/saved3.pkl', 'rb') as f:  # open a text file\n",
    "    saved = pickle.load( f) # serialize the list\n",
    "    dqn= saved[\"dqn\"].to(device)\n",
    "\"\"\"from torchrl.modules import NoisyLinear\n",
    "DQN = torch.nn.Sequential(NoisyLinear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          NoisyLinear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          NoisyLinear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          NoisyLinear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          NoisyLinear(nb_neurons, n_action)).to(device)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "agent = dqn_agent(config, DQN)\n",
    "agent.model = dqn\n",
    "agent.target_model = dqn\n",
    "agent.train(env, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_hiv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 36\u001b[0m Pagent \u001b[38;5;241m=\u001b[39m \u001b[43mProjectAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m agent\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     38\u001b[0m Pagent\u001b[38;5;241m.\u001b[39mdqn \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mmodel\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mProjectAgent.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) :\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m\u001b[43menv_hiv\u001b[49m\u001b[38;5;241m.\u001b[39mHIVPatient(domain_randomization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env_hiv' is not defined"
     ]
    }
   ],
   "source": [
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    state= np.log(state+1e-9)\n",
    "    if len(torch.Tensor(state).shape)==1 :\n",
    "        state = torch.Tensor(state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).to(device))\n",
    "        return torch.argmax(Q, dim=1).cpu()\n",
    "class ProjectAgent:\n",
    "\n",
    "    def __init__(self) :\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env =env_hiv.HIVPatient(domain_randomization=False)\n",
    "        self.config = config\n",
    "        self.dqn = None\n",
    "    def act(self, observation: np.ndarray, use_random: bool = False) -> int:\n",
    "        return greedy_action(self.dqn, observation)\n",
    "\n",
    "    def save(self, path=\"\"):\n",
    "        serialized= {\"dqn\":self.dqn.cpu(), \"config\":self.config}\n",
    "        with open('DQNAGENTS/saved3.pkl', 'wb') as f:  # open a text file\n",
    "            pickle.dump(serialized, f) # serialize the list\n",
    "    def load(self):\n",
    "        with open('DQNAGENTS/saved3.pkl', 'rb') as f:  # open a text file\n",
    "            saved = pickle.load( f) # serialize the list\n",
    "        self.dqn= saved[\"dqn\"].to(self.device)\n",
    "        try : \n",
    "            x,_ = self.env.reset()\n",
    "            self.act(x)\n",
    "        except : \n",
    "            raise Exception(\"Actor incompatible with environnement\")\n",
    "    \n",
    "    def train(self):\n",
    "        return self.agent.train(self.env,self.config['epochs'])\n",
    "Pagent = ProjectAgent()\n",
    "agent.model = agent.model.eval()\n",
    "Pagent.dqn = agent.model\n",
    "\n",
    "Pagent.load()\n",
    "from evaluate import evaluate_HIV, evaluate_HIV_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.516853e+10 3.138168e+10\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=Pagent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=Pagent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")\n",
    "print(\"{:e}\".format(score_agent),\"{:e}\".format(score_agent_dr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "names = os.listdir(\"DQNAGENTS/\")\n",
    "models= []\n",
    "for name in names :\n",
    "    with open(\"DQNAGENTS/\"+name, 'rb') as f:  # open a text file\n",
    "        models.append(pickle.load( f)['dqn']) # serialize the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProjectAgent:\n",
    "    def __init__(self) :\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.env =env_hiv.HIVPatient(domain_randomization=False)\n",
    "        self.config = config\n",
    "        self.models = []\n",
    "    def act(self, observation: np.ndarray, use_random: bool = False) -> int:\n",
    "        observation = (torch.Tensor(observation).to(torch.float32)+1e-9).log()\n",
    "        logits = [m(observation) for m in self.models[:3]]\n",
    "        #logits =[models[0](observation)]+[models[2](observation)]\n",
    "        logits =torch.softmax(torch.stack(logits,0),1).mean(0)\n",
    "        return torch.argmax(logits).item()\n",
    "\n",
    "    def save(self, path=\"\"):\n",
    "        serialized= {\"models\" :self.models}\n",
    "        with open(path+'models.pkl', 'wb') as f:  # open a text file\n",
    "            pickle.dump(serialized, f) # serialize the list\n",
    "    def load(self):\n",
    "        with open('models.pkl', 'rb') as f:  # open a text file\n",
    "            saved = pickle.load( f) # serialize the list\n",
    "        self.models= saved[\"models\"]\n",
    "        try : \n",
    "            x,_ = self.env.reset()\n",
    "            self.act(x)\n",
    "        except : \n",
    "            raise Exception(\"Actor incompatible with environnement\")\n",
    "agent = ProjectAgent()\n",
    "agent.models= models\n",
    "agent.save()\n",
    "agent.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.877342e+10 2.528770e+10\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Keep the following lines to evaluate your agent unchanged.\n",
    "score_agent: float = evaluate_HIV(agent=agent, nb_episode=1)\n",
    "score_agent_dr: float = evaluate_HIV_population(agent=agent, nb_episode=15)\n",
    "with open(file=\"score.txt\", mode=\"w\") as f:\n",
    "    f.write(f\"{score_agent}\\n{score_agent_dr}\")\n",
    "print(\"{:e}\".format(score_agent),\"{:e}\".format(score_agent_dr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved2.pkl', 'saved0.pkl', 'saved3.pkl', 'saved1.pkl']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
